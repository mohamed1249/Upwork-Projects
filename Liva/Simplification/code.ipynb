{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d69d5d7fbe34d1d8d3dff12e0acf359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\LAPTOP WORLD\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc9271299724f96adec4a8a4f45747e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b42c9803374767a89177d9080fd1ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8801ae21c84a48578d7c0b4f187151a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8688722ece03489d9e9fb89a07cda16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffb28bd2f88447439d8ebc3e06ccaff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/108 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f8a97ac594a4e799c69f3f70bb8f472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW\n",
    "\n",
    "# Define your custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"philippelaban/keep_it_simple\",force_download=True, resume_download=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = inputs[\"input_ids\"].squeeze()\n",
    "        attention_mask = inputs[\"attention_mask\"].squeeze()\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }\n",
    "\n",
    "# Custom training loop\n",
    "def train(model, dataset, optimizer, device):\n",
    "    model.train()\n",
    "    data_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Prepare your custom dataset\n",
    "with open(r\"wiki.full.aner.ori.train.dst\", 'r', encoding='utf-8') as file:\n",
    "        custom_texts = file.readlines()[:1000]\n",
    "\n",
    "# Prepare your custom dataset\n",
    "with open(r\"tune.8turkers.tok.turk.4\", 'r', encoding='utf-8') as file:\n",
    "        custom_texts += file.readlines()[:1000]\n",
    "\n",
    "dataset = CustomDataset(custom_texts)\n",
    "\n",
    "# Instantiate the model and optimizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"philippelaban/keep_it_simple\")\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Set up GPU training if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Train the model\n",
    "train(model, dataset, optimizer, device)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"trained_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query_text</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>snt_id</th>\n",
       "      <th>source_snt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G11.1</td>\n",
       "      <td>drones</td>\n",
       "      <td>2892036907</td>\n",
       "      <td>G11.1_2892036907_1</td>\n",
       "      <td>In the modern era of automation and robotics, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>G11.1</td>\n",
       "      <td>drones</td>\n",
       "      <td>2892036907</td>\n",
       "      <td>G11.1_2892036907_2</td>\n",
       "      <td>With the ever increasing number of unmanned ae...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>G11.1</td>\n",
       "      <td>drones</td>\n",
       "      <td>2892036907</td>\n",
       "      <td>G11.1_2892036907_3</td>\n",
       "      <td>Due to guidelines set by the governments regar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>G11.1</td>\n",
       "      <td>drones</td>\n",
       "      <td>2892036907</td>\n",
       "      <td>G11.1_2892036907_4</td>\n",
       "      <td>In an attempt to achieve the above mentioned t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>G11.1</td>\n",
       "      <td>drones</td>\n",
       "      <td>2892036907</td>\n",
       "      <td>G11.1_2892036907_5</td>\n",
       "      <td>Derived from the classic image classification ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>M9</td>\n",
       "      <td>Mechanisms of Muscle Hypertrophy</td>\n",
       "      <td>41</td>\n",
       "      <td>M9_41_3</td>\n",
       "      <td>Bodybuilders generally train with moderate loa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>M9</td>\n",
       "      <td>Mechanisms of Muscle Hypertrophy</td>\n",
       "      <td>41</td>\n",
       "      <td>M9_41_4</td>\n",
       "      <td>Powerlifters, on the other hand, routinely tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>M9</td>\n",
       "      <td>Mechanisms of Muscle Hypertrophy</td>\n",
       "      <td>41</td>\n",
       "      <td>M9_41_5</td>\n",
       "      <td>Although both groups are known to display impr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>M9</td>\n",
       "      <td>Mechanisms of Muscle Hypertrophy</td>\n",
       "      <td>41</td>\n",
       "      <td>M9_41_6</td>\n",
       "      <td>It has been shown that many factors mediate th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>M9</td>\n",
       "      <td>Mechanisms of Muscle Hypertrophy</td>\n",
       "      <td>41</td>\n",
       "      <td>M9_41_7</td>\n",
       "      <td>Therefore, the purpose of this paper is twofol...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>648 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    query_id                        query_text      doc_id   \n",
       "0      G11.1                            drones  2892036907  \\\n",
       "1      G11.1                            drones  2892036907   \n",
       "2      G11.1                            drones  2892036907   \n",
       "3      G11.1                            drones  2892036907   \n",
       "4      G11.1                            drones  2892036907   \n",
       "..       ...                               ...         ...   \n",
       "643       M9  Mechanisms of Muscle Hypertrophy          41   \n",
       "644       M9  Mechanisms of Muscle Hypertrophy          41   \n",
       "645       M9  Mechanisms of Muscle Hypertrophy          41   \n",
       "646       M9  Mechanisms of Muscle Hypertrophy          41   \n",
       "647       M9  Mechanisms of Muscle Hypertrophy          41   \n",
       "\n",
       "                 snt_id                                         source_snt  \n",
       "0    G11.1_2892036907_1  In the modern era of automation and robotics, ...  \n",
       "1    G11.1_2892036907_2  With the ever increasing number of unmanned ae...  \n",
       "2    G11.1_2892036907_3  Due to guidelines set by the governments regar...  \n",
       "3    G11.1_2892036907_4  In an attempt to achieve the above mentioned t...  \n",
       "4    G11.1_2892036907_5  Derived from the classic image classification ...  \n",
       "..                  ...                                                ...  \n",
       "643             M9_41_3  Bodybuilders generally train with moderate loa...  \n",
       "644             M9_41_4  Powerlifters, on the other hand, routinely tra...  \n",
       "645             M9_41_5  Although both groups are known to display impr...  \n",
       "646             M9_41_6  It has been shown that many factors mediate th...  \n",
       "647             M9_41_7  Therefore, the purpose of this paper is twofol...  \n",
       "\n",
       "[648 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json('simpletext_task3_train (1) (1) (1) (1).json')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///C:/python/projects/Upwork-Projects/Liva/Simplification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\laptop world\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "ERROR: file:///C:/python/projects/Upwork-Projects/Liva/Simplification does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/feralvam/easse.git\n",
    "!cd easse\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easse.sari import corpus_sari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"path/to/trained_model_directory\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"path/to/trained_model_directory\")\n",
    "\n",
    "# Set up GPU training if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Example input text\n",
    "input_text = \"Input text goes here.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer.encode_plus(\n",
    "    input_text,\n",
    "    add_special_tokens=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=128,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "input_ids = inputs[\"input_ids\"].squeeze().to(device)\n",
    "attention_mask = inputs[\"attention_mask\"].squeeze().to(device)\n",
    "\n",
    "# Generate output from the model\n",
    "outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=128)\n",
    "\n",
    "# Decode the generated output\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated output\n",
    "print(\"Generated text:\", generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
