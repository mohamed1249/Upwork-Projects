{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e612435",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Artificial Neural Networks 2 - Multi Layer Perceptrons or Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7782430-8d2f-4cbd-8905-84960f5dbeb6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This week's task:\n",
    "## Train a Neural Network to predict the grades of a metal alloy based on a series of parameters\n",
    "\n",
    "### Description\n",
    "\n",
    "During the metal manufacturing process, a range of parameters can affect the grade, and therefore the potential uses, of the resulting alloy. Given values for these parameters, we would like to be able to predict the grade of the resulting alloy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e35dd6f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The dataset\n",
    "\n",
    "The dataset included in the accompanying `Metal_Grade.csv` file is a slightly modified version of the dataset from this [link](https://www.kaggle.com/esotericazzo/metal-furnace-dataset) and includes 27 factors affecting the quality of manufactured alloys, along with the resulting grade for 620 cases. The file can be visualised with `pandas` as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f173679e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd #import pandas module\n",
    "\n",
    "#pd.set_option('display.max_rows', None) #uncomment this line to view all rows\n",
    "#pd.set_option('display.max_columns', None) #uncomment this line to view all columns\n",
    "\n",
    "metalDf = pd.read_csv(\"Metal_Grade.csv\") #read the file containing the data\n",
    "\n",
    "metalDf #visualise data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c60b1ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The SLP with multiple outputs\n",
    "\n",
    "### Architecture\n",
    "\n",
    "The first issue that we encounter when trying to apply the SLP to more complicated problems, such as the one described above, is the number of outputs. \n",
    "\n",
    "The SLP, as described in the previous lecture, has a single output, which can be converted to binary format by applying an activation function and a threshold. However, this is only suitable for regression with one output, or binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449cad0c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A way to extend the SLP to more complicated problems, is by simply increasing the number of outputs, as illustrated below for a SLP with 3 inputs:\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "<div>\n",
    "<img src=\"./Figures/slp_2a.png\" width=\"600\" align=\"center\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632b6a7f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The above figure introduces an alternative way of drawing neural networks, where:\n",
    "\n",
    "- An additional \"fictitious\" input with value 1 is considered.\n",
    "- The weights corresponding to this input are aquivalent to the biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433ab14b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, very often, this part is ommited, and networks are simply drawn as:\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "<div>\n",
    "<img src=\"./Figures/slp_2b.png\" width=\"600\" align=\"center\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059f0cac",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Moreover, notice that:\n",
    "\n",
    "- Every input is connected to every output, creating a so called fully connected layer\n",
    "- Weights have two indices:\n",
    "    + the first is related to the input they are connected to and\n",
    "    + the second is related to the output.\n",
    "- Biases also have an index, since a different bias is defined for each output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91575a1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This architecture can be directly used for regression with multiple outputs. For classification problems, the outputs can be converted to values representing probabilities of the inputs belonging to each class through the use of a softmax activation function. \n",
    "\n",
    "The expression for the softmax function is: $$f\\left( \\mathbf{x} \\right)_i = \\dfrac{e^{x_i}}{\\Sigma_{j=1}^{n}e^{x_j}}$$ As an example, if the outputs of a MLP for classification using three classes are $y_1=12, y_2 = 16, y_3 = 17$, applying the softmax function will yield: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228945f1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    '''Softmax function'''\n",
    "    return np.exp(x)/np.sum(np.exp(x))\n",
    "\n",
    "y = np.array([12., 16., 17.])\n",
    "\n",
    "yProb = softmax(y);\n",
    "\n",
    "print(yProb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179095cd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The above can be interpreted as the input having 0.49% probability of belonging to class 1, 26.76% probability of belonging to class 2 and 72.74% probability of belonging to class 3. Very often we are only interested in knowing which class each input belongs to. This can be achieved by just extracting the class with the highest probability. Using numpy, this can be done as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2fe794",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "yClass = np.argmax(yProb) #argmax returns the index of the maximum element of an array\n",
    "print(yClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d75920",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mathematical expression\n",
    "\n",
    "For a general network with $n$ inputs and $m$ outputs, the operations performed by individual elements can be compactly described by the following expression:\n",
    "\n",
    "$ y_j = f\\left( \\Sigma_{i=1}^n w_{ij} x_j + b_j \\right)$\n",
    "\n",
    "where:\n",
    "\n",
    "+ $x_i$ are the inputs, for instance $x_1, x_2, \\dots$\n",
    "+ $y_j$ are the outputs, for instance $y_1, y_2, \\dots$\n",
    "+ $w_{ij}$ is the weight corresponding to input $i$ and output $j$\n",
    "+ $b_j$ is the bias corresponding to output $j$\n",
    "+ $f$ is the activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff262f7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The inputs weights, biases, and outputs in the above expression can be written in matrix form as:\n",
    "\n",
    "$$\\mathbf{x} = \\left[\t\n",
    "\t\\begin{array}{c }\n",
    "    \tx_1   \\\\\n",
    "     \tx_2   \\\\\n",
    "        \\vdots \\\\\n",
    "        x_n  \\end{array}\\right], \\mathbf{W} = \\left[\t\n",
    "\t\\begin{array}{c c c }\n",
    "    \tw_{11} & w_{12} & \\dots  & w_{1m} \\\\\n",
    "     \tw_{21} & w_{22} & \\dots  & w_{2m} \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        w_{n1} & w_{n2} & \\dots  & w_{nm}\\end{array}\\right]$$\n",
    "        \n",
    "$$\\mathbf{b} = \\left[\t\n",
    "\t\\begin{array}{c }\n",
    "    \tb_1   \\\\\n",
    "     \tb_2   \\\\\n",
    "        \\vdots \\\\\n",
    "        b_m  \\end{array}\\right], \\mathbf{y} = \\left[\t\n",
    "\t\\begin{array}{c }\n",
    "    \ty_1   \\\\\n",
    "     \ty_2   \\\\\n",
    "        \\vdots \\\\\n",
    "        y_m  \\end{array}\\right]$$\n",
    "\n",
    "Then, the mathematical expression describing the SLP can be re-written as:\n",
    "\n",
    "$$ \\mathbf{y}^T = f\\left( \\mathbf{x}^T \\mathbf{W} + \\mathbf{b}^T \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaf4ab3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "As an example, consider a set of points in the 2D plane, stored in the accompanying `plane_points_3_classes.txt` and `plane_points_3_classes_labels.txt` files. They points are divided into three classes and can be loaded and visualised as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983ccb20",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np #import numpy\n",
    "from matplotlib import pyplot as plt #import pyplot\n",
    "\n",
    "#load points and labels\n",
    "x3 = np.loadtxt('plane_points_3_classes.txt')\n",
    "labels3 = np.loadtxt('plane_points_3_classes_labels.txt')\n",
    "\n",
    "#use np.argwhere to find the indices of the points belonging to each class\n",
    "class0 = np.argwhere(labels3==0)\n",
    "class1 = np.argwhere(labels3==1)\n",
    "class2 = np.argwhere(labels3==2)\n",
    "\n",
    "#plot the points belonging to each class using a different color\n",
    "plt.plot(x3[class0,0],x3[class0,1],'.r')\n",
    "plt.plot(x3[class1,0],x3[class1,1],'.k')\n",
    "plt.plot(x3[class2,0],x3[class2,1],'.b')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eab9e43",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Training with tensorflow\n",
    "\n",
    "Next a SLP with three outputs and a sofmax activation function can be trained to classify the points using `keras`. The syntax is almost identical to the one used in the previous lecture, with three differences:\n",
    "+ Three outputs are used to match the number of classes\n",
    "+ The `activation` option is set to `softmax`, to convert the outputs to probabilities\n",
    "+ `SparseCategoricalEntropy` is used as a loss. This option is suitable for classification with multiple classes, where the labels are inegers corresponding to the class number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa322245",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf #import tensorflow\n",
    "\n",
    "modelSLP3 = tf.keras.Sequential() #create sequential model\n",
    "\n",
    "#add layer with 3 outputs and softmax activation\n",
    "modelSLP3.add(tf.keras.layers.Dense(3,activation='softmax'))\n",
    "\n",
    "#compile model, SparseCategoricalCrossentropy is used as a loss\n",
    "modelSLP3.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics='accuracy')\n",
    "\n",
    "#train the model for 100 epochs using the training data\n",
    "modelSLP3.fit(x3, labels3, epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1024856e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Confusion matrix\n",
    "\n",
    "Results can be visualised through a confusion matrix, using the same commands as shown last week. Notice that the `argmax` function is used to convert predicted probabilities to class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69dc176",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay #import confusion matrix related classes\n",
    "\n",
    "#use model to predict the labels of the training data as probabilities and convert to integers\n",
    "labelsPred = modelSLP3.predict(x3)\n",
    "labelsPred = np.argmax(labelsPred,axis=1)\n",
    "\n",
    "#create confusion matrix\n",
    "confusionMatrix3=confusion_matrix(labels3,labelsPred)\n",
    "\n",
    "#create and show confusion matrix plot\n",
    "confusionMatrixPlot3 = ConfusionMatrixDisplay(confusion_matrix=confusionMatrix3)\n",
    "confusionMatrixPlot3.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4325f86c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Visualising the decision boundary\n",
    "\n",
    "To better understand the properties of the created models, it can be helpful to visualise the boundaries between classes, also termed 'decision boundaries'. To this end we will use the following function, which is a slightly modified version of the one provided in [Jon Charests'notebook](https://jonchar.net/notebooks/Artificial-Neural-Network-with-Keras/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6e1279",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def plotDecisionBoundary(model,limx=[-1,1],limy=[-1,1],resolution=200, colormap = 'RdBu'):\n",
    "    '''Function to plot the decision boundary of a tensorflow model as a contour plot.\n",
    "       model is the model, which should have a predict method\n",
    "       limx, limy are the limits of the plot in x and y\n",
    "       resolution is the resolution of the plot in terms of the number of points used per direction\n",
    "       colormap is the colormap to be used for the contour plot\n",
    "       The function returns a figure and an axis object'''\n",
    "    \n",
    "    #create figure\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    \n",
    "    #create linspaces with the x and y coordinates of the points to be used for the contour plot\n",
    "    #the limits and resolution are set to the user provided values\n",
    "    xPoints = np.linspace(limx[0], limx[1], resolution)\n",
    "    yPoints = np.linspace(limy[0], limy[1], resolution)\n",
    "    \n",
    "    #create a meshgrid from the provided\n",
    "    xx, yy = np.meshgrid(xPoints, yPoints)\n",
    "    \n",
    "    #use model to predict the labels of the generated points\n",
    "    #ravel and c_ are used to bring the coordinates in the correct shape\n",
    "    modelPred = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    #check shape of the output, if the output is twodimensional and the size in the second dimension is more than 1\n",
    "    #then the ouput is assumed to be provided in terms of probabilities and is converted to class indices using the argmax function\n",
    "    if len(modelPred.shape)==2 and modelPred.shape[1]!=1:\n",
    "        modelPred = np.argmax(modelPred,axis=1)\n",
    "    else:\n",
    "    #if the output is either onedimensional or the second dimension has size one, \n",
    "    #then it is assumed to be labels for binary classification and a 0.5 threshold is applied to convert to binary format\n",
    "        modelPred = modelPred>0.5\n",
    "    \n",
    "    #reshape the labels to the shape of xx and yy so that they can be used for a contour plot\n",
    "    z = modelPred.reshape(xx.shape)\n",
    "\n",
    "    #create contour plot\n",
    "    ax.contourf(xx,yy,z,cmap=colormap,alpha=0.5)\n",
    "    \n",
    "    #return figure and axis\n",
    "    return fig,ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625d2ace",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The function can be used to plot the decision boundary of the model, along with the points corresponding to the different classes as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad33dc5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#call provided function, with modelSLP3 as input\n",
    "#this creates a contour plot with different colors used for each class\n",
    "fig, ax = plotDecisionBoundary(modelSLP3)\n",
    "\n",
    "#plot points of each class using a different color\n",
    "ax.plot(x3[class0,0],x3[class0,1],'.r')\n",
    "ax.plot(x3[class1,0],x3[class1,1],'.k')\n",
    "ax.plot(x3[class2,0],x3[class2,1],'.b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b2ae9c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Multi Layer Perceptron\n",
    "\n",
    "### Architecture\n",
    "\n",
    "The above can be further extended to create more complex architectures termed Multi Layer Perceptrons (MLPs) or feedforward neural networks. These consist of multiple layers, each similar to a SLP, placed one after the other, with the outputs of each previous layer serving as an input for the next. All layers apart from the input and output ones are termed hidden layers. In the following figure, an example of a simple such architecture, consisting of one hidden layer, in addition to the input and output layer, is illustrated:\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "<div>\n",
    "<img src=\"./Figures/mlp1.png\" width=\"600\" align=\"center\">\n",
    "</div>\n",
    "\n",
    "Next the individual elements of a MLP are described in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11dd8a3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Inputs\n",
    "\n",
    "The multi-dimensional inputs $x_1, x_2, \\dots, x_n$ of the MLP are similar to the ones of the SLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fff6b4c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Hidden layers\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "<div>\n",
    "<img src=\"./Figures/mlp_hidden_layers.png\" width=\"1000\" align=\"center\">\n",
    "    <br/><a align=\"center\">MLP with 1, 2 and 3 hidden layers respectively</a>\n",
    "</div>\n",
    "\n",
    "All layers apart from the input and output ones are termed 'hidden'. Each of the hidden layers includes a number of units, each with a set of weights and biases, as well as an activation function. As will be also mentioned later, networks with at least one hidden layer can represent nonlinear functions for regression, or nonlinear boundaries between classes for classification. However, the number of hidden layers used, can affect the properties of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd47e5f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Hidden units\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "<div>\n",
    "<img src=\"./Figures/mlp_hidden_units.png\" width=\"1000\" align=\"center\">\n",
    "    <br/><a align=\"center\">MLP with 1 hidden layer and 3, 5, and 7 hidden units respectively</a>\n",
    "</div>\n",
    "\n",
    "Each hidden layer consists of a number of units, connected to all outputs of the previous layer and acting as inputs for the next layer. The number of units in each hidden layer does not have to be equal to the number of inputs or outputs of the network, however it might affect the properties of the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b1290e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Weights, summing and bias\n",
    "\n",
    "In each layer of a MLP, the outputs of previous layers are used as input. Similar to the SLP, these inputs are weighted and summed at each unit and biases are added. The process is in general similar to the SLP case, however a larger number of weights and biases needs to be deterined as a result of the larger number of layers and units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c882318f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Activation\n",
    "\n",
    "Similar to the SLP, activation is performed after weighting and summing at each layer. Some activation functions used in MLPs are given below:\n",
    "\n",
    "|     Activation function    |                Expression                    |                                 Graph                             |\n",
    "|----------------------------|----------------------------------------------|------------------------------------------------------------------|\n",
    "|Rectified Linear Unit (ReLU)|$$ f\\left( x \\right) = \\max\\left(x,0\\right) $$|<img src=\"./Figures/ReLU.png\" width=\"300\" align=\"center\"> |\n",
    "|Softsign                    |$$ f\\left( x \\right) = \\frac{x}{\\lvert x \\rvert+1} $$|<img src=\"./Figures/softsign.png\" width=\"300\" align=\"center\"> |\n",
    "|Softplus                    |$$ f\\left( x \\right) = \\log{\\left(e^{x}+1 \\right)} $$|<img src=\"./Figures/softplus.png\" width=\"300\" align=\"center\"> |\n",
    "\n",
    "In modern neural networks, the most popular of these is probably the Rectified Linear Unit (ReLU)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84685d3a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Output layer\n",
    "\n",
    "In the output layer, weighting and summing is performed similar to hidden layers.\n",
    "\n",
    "##### Activation\n",
    "\n",
    "The activation function used for output layers, might be slightly different than hidden layers. For example:\n",
    "\n",
    "+ **For regression problems**, a linear activation might be used.\n",
    "+ **For classification problems**, a softmax activation function can be used to convert each output to a probability of the input belonging to a certain class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319f4381",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mathematical expression\n",
    "\n",
    "In what follows, we will use superscritps in brackets to denote elements of the $i$th layer of a network, where $i$ assumes a value of 0 for the input layer, 1 for the first hidden layer etc. For instance:\n",
    "\n",
    "- $f^{\\left(2\\right)}$ denotes the activation function of the second hidden layer\n",
    "- $\\mathbf{W}^{\\left( 1 \\right)}$ denotes the matrix of weights of the first hidden layer\n",
    "\n",
    "Using this notation, the output of the first hidden layer of a MLP will be:\n",
    "\n",
    "$$\\left(\\mathbf{y}^{\\left( 1 \\right)}\\right)^T = f^{\\left(1 \\right)}\\left( \\mathbf{x}^T \\mathbf{W}^{\\left( 1 \\right)} + \\left(\\mathbf{b}^{\\left( 1 \\right)}\\right)^T \\right)$$\n",
    "\n",
    "This is subsequently used as input for the second layer, yielding the expression:\n",
    "\n",
    "$$ \\left(\\mathbf{y}^{\\left( 2 \\right)}\\right)^T = f^{\\left(2 \\right)}\\left( \\left(\\mathbf{y}^{\\left( 1 \\right)}\\right)^T \\mathbf{W}^{\\left( 2 \\right)} + \\left(\\mathbf{b}^{\\left( 2 \\right)}\\right)^T \\right) = f^{\\left(2 \\right)}\\left( \\left( f^{\\left(1 \\right)}\\left( \\mathbf{x}^T \\mathbf{W}^{\\left( 1 \\right)} + \\left(\\mathbf{b}^{\\left( 1 \\right)}\\right)^T \\right) \\right)^T \\mathbf{W}^{\\left( 2 \\right)} + \\left(\\mathbf{b}^{\\left( 2 \\right)}\\right)^T \\right)$$\n",
    "\n",
    "The process is then repeated for all layers of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d3eb37",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67719fcd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Loss function\n",
    "\n",
    "As also mentioned in last week's lecture, different loss functions can be used to measure the discrepancy between predicted and actual labels. For classification problems, a commonly used loss, also employed in last week's computer lab, is cross entropy. For a binary classificartion problem, and a set of training labels $y^*_i$ and actual labels $y_i$, cross entropy can be defined as:\n",
    "\n",
    "$$E = -\\dfrac{1}{m} \\Sigma_{i=1}^m \\left[ y^*_i \\log{y_i} + \\left(1-y^*_i\\right) \\log{\\left( 1-y_i \\right) } \\right]$$\n",
    "\n",
    "where $m$ is the number of training data points. The definition can be extended to higher numebers of classes by considering a probability for each class, as mentioned above, and summing over the number of classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc09152",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Back propagation\n",
    "\n",
    "Weights of MLPs can be updated using extensions of the methods described in the previous lecture for SLPs. An necessary step for most methods, is the computation of the partial derivatives of the loss function with respect to the weights and biases, which can be computed using the chain rule, for instance:\n",
    "\n",
    "$$\\dfrac{\\partial E}{\\partial w_{ij}} = \\Sigma_{k=1}^m\\dfrac{\\partial E}{\\partial y_{k}} \\dfrac{\\partial y_k}{\\partial w_{ij}}$$\n",
    "\n",
    "where $m$ is the number of training data points.\n",
    "\n",
    "Since MLPs involve more layers than SLPs the computation of $\\dfrac{\\partial y_k}{\\partial w_{ij}}$ is more involved, requiring the use of the chain rule multiple times. The process of using the chain rule recursively, is termed **back propagation** as oposed to **forward propagation**, which is the process of pasing inputs through the MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2cef7e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "\n",
    "To illustrate the differences between SLPs and SLPc and demonstrate how SLPs can be trained with `Keras`, lets consider a simple example with points in the two dimensional plane. The coordinates and labels ofthe points are stored in the accompanying `plane_points_nal.txt` and `plane_points_nl_labels.txt`. The labels have been assigned based on whether the points are above or below the curve defined by:\n",
    "\n",
    "$$y = 8x^3-3x$$\n",
    "\n",
    "Thus, the decision boundary essentially corresponds to that curve.\n",
    "\n",
    "The points can be loaded and visualised along with the curve as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78120b06",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#import numpy and pyplot\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#load points and labels from files\n",
    "x=np.loadtxt('plane_points_nl.txt')\n",
    "labels = np.loadtxt('plane_points_nl_labels.txt')\n",
    "\n",
    "#use np.argwhere to find the indices of the points belonging to each class\n",
    "class0 = np.argwhere(labels==1)\n",
    "class1 = np.argwhere(labels==0)\n",
    "\n",
    "#Generate coordinates of points along the decision boundary\n",
    "xDB = np.linspace(-0.73,0.73,1000)\n",
    "yDB = 8*xDB**3-3*xDB\n",
    "\n",
    "#plot classes using different colors\n",
    "plt.plot(x[class0,0],x[class0,1],'.b')\n",
    "plt.plot(x[class1,0],x[class1,1],'.r')\n",
    "\n",
    "#plot decision boundary\n",
    "plt.plot(xDB,yDB,'--k',label='Decision boundary')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbbffae",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In contrast to the cases considered so far, the points are clearly not linearly separable!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8898ea4f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Classification with a SLP\n",
    "\n",
    "As a first test, we can try to train a SLP for this problem. However, since the data is not linearly separable, we would expect it to perform poorly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bf6335",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#create a sequential model for the slp\n",
    "modelSLP = tf.keras.Sequential()\n",
    "\n",
    "#add a dense layer with 1 output and a sigmoid activation function\n",
    "modelSLP.add(tf.keras.layers.Dense(1,\n",
    "                                   activation = 'sigmoid'))\n",
    "\n",
    "#compile model\n",
    "modelSLP.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics='accuracy')\n",
    "\n",
    "#train model for 100 epochs using the data from the points \n",
    "modelSLP.fit(x, labels, epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd2e9bd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Results can be visualised in terms of the decision boundary as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ecce93",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#plot decision boundary using provided function\n",
    "fig, ax = plotDecisionBoundary(modelSLP)\n",
    "\n",
    "#plot data points\n",
    "ax.plot(x[class0,0],x[class0,1],'.b')\n",
    "ax.plot(x[class1,0],x[class1,1],'.r')\n",
    "\n",
    "#plot actual decision boundary\n",
    "ax.plot(xDB,yDB,'--k',label='Decision boundary')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc8eab9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We observe that:\n",
    "+ The SLP performs poorly in terms of the achieved accuracy, regardless of the number of training epochs used\n",
    "+ The decision boundary for the SLP is a linear approximation of the actual decision boundary, which however is not enough to provide accurate results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24a865c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training a MLP\n",
    "\n",
    "Next, let's try to train a MLP to improve the above results. Our network will consist of:\n",
    "\n",
    "+ One hidden layer with 8 hidden units and ReLU activation\n",
    "+ One output layer with 2 outputs and softmax activartion to convert outputs to probabilities\n",
    "\n",
    "Using `keras`, this can be accomplished in a very similar way to what was shown so far for the SLP with multiple outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2320f44d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#create a sequential model for the mlp\n",
    "modelMLP = tf.keras.Sequential()\n",
    "\n",
    "#add a dense layer with 8 outputs and a ReLU activation\n",
    "modelMLP.add(tf.keras.layers.Dense(8,\n",
    "                                   activation = 'ReLU'))\n",
    "\n",
    "#add a dense layer with 2 outputs and softmax activation to convert outputs to probabilities\n",
    "modelMLP.add(tf.keras.layers.Dense(2,\n",
    "                                   activation = 'softmax'))\n",
    "\n",
    "#compile and train model for 100 epochs  \n",
    "modelMLP.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                 metrics='accuracy')\n",
    "\n",
    "modelMLP.fit(x, labels, epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bbeeed",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualising the decision boundary\n",
    "\n",
    "Results can be visualised in terms of the decision boundary as in the SLP case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfaf6ed",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#plot decision boundary using provided function\n",
    "fig, ax = plotDecisionBoundary(modelMLP)\n",
    "\n",
    "#plot data points\n",
    "ax.plot(x[class0,0],x[class0,1],'.b')\n",
    "ax.plot(x[class1,0],x[class1,1],'.r')\n",
    "\n",
    "#plot actual decision boundary\n",
    "ax.plot(xDB,yDB,'--k',label='Decision boundary')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8401edf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training\n",
    "\n",
    "Based on the above it is clear that if a large enough number of hidden units and/or hidden layers are used, nonlinear decision boundaries can be approximated. In fact, it can be shown that MLPs with a single hidden layer can approximate almost any function with an arbitrary degree of accuracy, provided that enough hidden units are used. More generally, the ability of the model to represent different functions is termed **capacity**. For MLPs, the model's capacity is affected by parameters such as the number of hidden units and hidden layers, learning rate and number of training epochs.\n",
    "\n",
    "While the above is very important for the general applicability of neural networks, it does not provide any guidance on how to select the number of hidden units and/or hidden layers for a given problem. In practice, these parameters are determined through experimentation and/or experience. The next subsections introduce some important concepts that are important for the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e55b12",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Generalisation\n",
    "\n",
    "So far we have only tested the error introduced by SLPs/MLPs when reproducing the training data, i.e. the **training error**. However, the main goal of machine learning algorithms is to make accurate predictions for **previously unseen data**, a property called **generalisation**. The extent to which this goal is achieved can be quantified through the so called **generalisation error** or **test error**, i.e. the expected error for a new, unseen input. A simple way of estimating this error is by computing the error for a new dataset, independent from the training set, termed the **test set**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c37d216",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Underfitting/Overfitting\n",
    "\n",
    "Considering the above, the capacity of machine learning models should be chosen to satisy two requirements:\n",
    "\n",
    "1. **Small training error:** a high enough capacity should be used to represent the function/decision boundary for a given problem. If the capacity of the model is too low, it will lead to large values for both the training and test error, a phenomenon termed **underfitting**. As an example of underfitting, consider the case of the SLP used for the previous example. In that case, the capacity of the model was too low for the given problem, leading to poor results.\n",
    "2. **Small discrepancy between the training and test error:** the capacity of the model should not be too high, otherwise the model might 'memorise' every single data point in the data set, rather than learning the required function or decision boundary. This leads to a phenomenon called **overfitting**, where low training errors do not translate to low test/generalisation error. As will be shown in the next example, overfitting can lead to particularly poor results in the presence of noise.\n",
    "\n",
    "The typical relationship between model capacity and the different types of error is illustrated in the following figure: \n",
    "<br />\n",
    "\n",
    "<div>\n",
    "<img src=\"./Figures/overfitting.png\" width=\"600\" align=\"center\">\n",
    "    <br/><a align=\"center\">Relationship between capacity and error. Figure inspired from (Goodfellow et al., 2016).</a>\n",
    "</div>\n",
    "\n",
    "A good training strategy, should, in general, aim at determining the optimal capacity for the model, i.e. the one that yields the smallest possible training error, without deteriorating the test error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73abcf4d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "To illustrate how overfitting can lead to poor generalisation, we will consider a set of points similar to the one used in the previous example, but with added noise, which might also be present in real data. The data is included in the `plane_points_noise1.txt` and `plane_points_noise_labels1.txt` files and can be visualised as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f73308",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "xOF=np.loadtxt('plane_points_noise.txt')\n",
    "labelsOF = np.loadtxt('plane_points_noise_labels.txt')\n",
    "\n",
    "class0OF = np.argwhere(labelsOF==1)\n",
    "class1OF = np.argwhere(labelsOF==0)\n",
    "\n",
    "plt.plot(xOF[class0OF,0],xOF[class0OF,1],'.b')\n",
    "plt.plot(xOF[class1OF,0],xOF[class1OF,1],'.r')\n",
    "plt.plot(xDB,yDB,'--k',label='Decision boundary')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3072084a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As a first step, we will split the data into a training and test set. This can be done automatically, using the `train_test_split` function from the `sklearn` package. The function requires, apart from the initial datapoints and labels, the percentage of data to be used for the training set. As output, it returns two different sets of datapoints and labels that can be used for training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762de181",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xTrain, xTest, labelsTrain, labelsTest = train_test_split(xOF,labelsOF, train_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c628e84",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, we will employ a model with very high capacity to reduce the training error as much as possible. To this end, we will create a model with 4 hidden layers consisting of 100 hidden units each and we will train for a very high number of epochs (1000):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a1a9f0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "modelOF = tf.keras.Sequential()\n",
    "\n",
    "\n",
    "modelOF.add(tf.keras.layers.Dense(100,\n",
    "                                  activation = 'relu'))\n",
    "\n",
    "modelOF.add(tf.keras.layers.Dense(100,\n",
    "                                  activation = 'relu'))\n",
    "\n",
    "modelOF.add(tf.keras.layers.Dense(100,\n",
    "                                  activation = 'relu'))\n",
    "\n",
    "modelOF.add(tf.keras.layers.Dense(100,\n",
    "                                  activation = 'relu'))\n",
    "\n",
    "modelOF.add(tf.keras.layers.Dense(2,\n",
    "                                  activation = 'softmax'))\n",
    "\n",
    "modelOF.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics='accuracy')\n",
    "\n",
    "\n",
    "modelOF.fit(xTrain, \n",
    "            labelsTrain, \n",
    "            epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005666b2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As seen above, a very good training error can be obtained, with accuracy in the training set exceeding 90%. However, this accuracy does not necessarily translate to good generalisation. To illustrate this, we can evaluate the loss and accuracy over the test set, using the evaluate method of the model object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f526a15",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "testLoss, testAcc = modelOF.evaluate(xTest,  labelsTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeea67b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As seen above, the test accuracy is significantly lower than the training error. Some further insight can be obtained by plotting the decision boundary for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30ceaf7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plotDecisionBoundary(modelOF)\n",
    "\n",
    "class0Train = np.argwhere(labelsTrain==1)\n",
    "class1Train = np.argwhere(labelsTrain==0)\n",
    "\n",
    "ax.plot(xTrain[class0Train,0],xTrain[class0Train,1],'.b')\n",
    "ax.plot(xTrain[class1Train,0],xTrain[class1Train,1],'.r')\n",
    "ax.plot(xDB,yDB,'--k',label='Decision boundary')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639b7757",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As seen above, the MLP has essentially memorised almost every single point in the training set, including the noise, rather than learning the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28c7d4c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "In order to avoid overfitting, hyperparameters, such as the number of hidden layers, should be selected to minimise the generalisation error, rather than the training error. However, the test error cannot be directly used to guide the process since it might bias the model. To avoid this, a third data set is typically introduced, termed the **validation set**, based on which hyperparameters are selected.Then, hyperparameters can be determined either manually, or in an automated way, based on a process similar to the following:\n",
    "\n",
    "+ Divide available data into test, validation and training data sets.\n",
    "+ Define a set of possible values for all hyperparameters to be determined. For instance, possible values for the number of hidden units might be: $[10, 20, 50, 100, 200, 500, 1000]$.\n",
    "+ For each possible combination of parameters:\n",
    "    + Create and train a model.\n",
    "    + Evaluate the error of this model over the validation set.\n",
    "    + For the first model: save the model and resulting error as the optimal model\n",
    "    + For every subsequent model: if the error is smaller than the optimal model, replace the optimal with the current model.\n",
    "+ Once all potential combinations have been tested, evaluate the error of the optimal model over the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f91e48",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "To illustrate the above process, consider the dataset from the previous example. First we further split the training set into training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f97abda",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xTrain, xVal, labelsTrain, labelsVal = train_test_split(xTrain,labelsTrain, train_size=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaaa01e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, we specify the sets of possible values for the model hyperparameters. In this case, we will only modify the number of hidden units for all hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0ac958",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "hiddenUnits = [2,5,10,20,50,100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4d3dd4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The sets of hidden units can be looped, with a model created and trained for each parameter value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82686f89",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#loop list with possible numbers of hidden units\n",
    "for units in hiddenUnits:\n",
    "    #create model\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    #add 4 layers with different numbers of hidden units\n",
    "    model.add(tf.keras.layers.Dense(units,\n",
    "                                      activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dense(units,\n",
    "                                      activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dense(units,\n",
    "                                      activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dense(units,\n",
    "                                      activation = 'relu'))\n",
    "\n",
    "    #add final dense layer with softmax activation\n",
    "    model.add(tf.keras.layers.Dense(2,\n",
    "                                    activation = 'softmax'))\n",
    "\n",
    "    #compile and fit model for training set\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics='accuracy')\n",
    "\n",
    "    model.fit(xTrain, \n",
    "                labelsTrain, \n",
    "                epochs=1000)\n",
    "    \n",
    "    #evaluate model for validation set\n",
    "    lossVal, accVal = model.evaluate(xVal,  labelsVal)\n",
    "    \n",
    "    if units==hiddenUnits[0]:\n",
    "        #if this is the first model tested, save model, error, and accuracy\n",
    "        modelOpt=model\n",
    "        lossOpt = lossVal\n",
    "        accOpt = accVal\n",
    "        unitsOpt = units\n",
    "    else:\n",
    "        #for every subsequent model check accuracy, if higher than current optimal model then replace optimal\n",
    "        if accVal>accOpt:\n",
    "            modelOpt=model\n",
    "            lossOpt = lossVal\n",
    "            accOpt = accVal\n",
    "            unitsOpt = units\n",
    "\n",
    "#print details of optimal model\n",
    "print('Best loss over test set: ', lossOpt)\n",
    "print('Best accuracy over validation set: ', accOpt)\n",
    "print('Number of units selected: ', unitsOpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7ca84e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The selected number of hidden layers is much smaller than the original model! The performance of the optimal model over the test set can be evaluated next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546000e6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "lossTest, accTest = modelOpt.evaluate(xTest,  labelsTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85b7d11",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The model achieves a smaller discrepancy between training and test error.\n",
    "\n",
    "Also, the decision boundary for the new model can be plotted as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef0938",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plotDecisionBoundary(modelOpt)\n",
    "\n",
    "#find indices of \n",
    "class0Train = np.argwhere(labelsTrain==1)\n",
    "class1Train = np.argwhere(labelsTrain==0)\n",
    "\n",
    "#plot data points, line used in first example, and line obtained after training\n",
    "ax.plot(xTrain[class0Train,0],xTrain[class0Train,1],'.b')\n",
    "ax.plot(xTrain[class1Train,0],xTrain[class1Train,1],'.r')\n",
    "ax.plot(xDB,yDB,'--k',label='Decision boundary')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72d5655",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It can be clearly seen that, despite the noisy data, the optimal model leads to a much smoother decision boundary, closer to the initial one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdeb560",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regularisation/Early stopping\n",
    "\n",
    "A variety of additional strategies can be employed to improve the generalisation error of MLPs and are collectively refered to as **regularisation** methods. In general, these methods introduce modifications, for instance in the loss function of training process, to improve the test error without necessarily altering the capacity of the model.\n",
    "\n",
    "While several such strategies are available, we will only describe one of the most commonly used for neural networks, named **early stopping**. The strategy is based on the observation that, during a typical training process, the training error will constantly decrease, while the validation error will follow a U shape, initially decreasing and increasing again after a certain point, as illustrated in the following figure:\n",
    "\n",
    "<div>\n",
    "<img src=\"./Figures/training_validation_error.png\" width=\"600\" align=\"center\">\n",
    "    <br/><a align=\"center\">Training and validation error during training. Figure taken from (Goodfellow et al., 2016).</a>\n",
    "</div>\n",
    "\n",
    "Based on that, the process should ideally be stopped at the point where the minimum validation error is reached. This can be accomplished by monitoring the validation error during training and terminating the process once an increase is observed for a few consecutive epochs. Then the weights achieved at the minimum value can be restored leading to an optimal model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff582c1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "To illustrate the effectiveness of early stopping, as well as how it can be implemented with `keras`, we will consider the previous example, with the same training, validation, testing set splits. The first modification required, is the definition of an early stopping callback. The following options are selected:\n",
    "\n",
    "+ The `monitor` parameter is set to `val_loss` to use the validation error as an early stopping criterion.\n",
    "+ The `patience` parameter is given a value of 10, meaning that if the error in the validation set is not improved after 10 consecutive epochs, training will stop.\n",
    "+ The `restore_best_weights` parameter is set to `True` to use the best weights achieved during training, rather than the last ones obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2dfe46",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#define Early stopping callback\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299183c8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to monitor the training and validation error, an additional class is imported from the [`livelossplot`](https://p.migdal.pl/livelossplot/#installation) module. The module does not come preinstalled with Anaconda, therefore it has to be installed by running the following command in an Anaconda (Windows) or regular (Mac) command line:\n",
    "\n",
    "```\n",
    "pip install livelossplot\n",
    "```\n",
    "\n",
    "Then, the required class, named `PlotLossesKeras` can be imported as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0d7031",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from livelossplot import PlotLossesKeras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea0cfcc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The model can be created and compiled in the same way as the one used to demonstrate overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddecf138",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "modelES = tf.keras.Sequential()\n",
    "\n",
    "\n",
    "modelES.add(tf.keras.layers.Dense(100,\n",
    "                                  activation = 'relu'))\n",
    "\n",
    "modelES.add(tf.keras.layers.Dense(100,\n",
    "                                  activation = 'relu'))\n",
    "\n",
    "modelES.add(tf.keras.layers.Dense(1000,\n",
    "                                  activation = 'relu'))\n",
    "\n",
    "modelES.add(tf.keras.layers.Dense(100,\n",
    "                                  activation = 'relu'))\n",
    "\n",
    "modelES.add(tf.keras.layers.Dense(2,\n",
    "                                  activation = 'softmax'))\n",
    "\n",
    "modelES.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8176e0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, the model can be fitted using the `fit` method. Notice that the following additional arguments are passed:\n",
    "\n",
    "+ The optional argument `validation_data` is used to provide the validation data and labels.\n",
    "+ The `callbacks` argument is used to pass the imported `PlotLossesKeras` object and the previously defined `callback` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee74a61",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "modelES.fit(xTrain, \n",
    "            labelsTrain,\n",
    "            validation_data=(xVal, labelsVal),\n",
    "            callbacks=[PlotLossesKeras(), callback], \n",
    "            epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c7f664",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `PlotLossesKeras` callback allows to monitor the training and validation errors in real time, while, through the use of early stopping, the difference between validation and training error can be decreased. Notice that this is achieved despite the fact that the capacity of the model used is probably to high for the current problem.\n",
    "\n",
    "The error and accuracy for the test set can be evaluated as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1902b8a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "lossTest, accTest = modelES.evaluate(xTest,  labelsTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c1414b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The decision boundary for the new model can be visualised as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edfe3cb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plotDecisionBoundary(modelES)\n",
    "\n",
    "ax.plot(xOF[class0OF,0],xOF[class0OF,1],'.b')\n",
    "ax.plot(xOF[class1OF,0],xOF[class1OF,1],'.r')\n",
    "ax.plot(xDB,yDB,'--k',label='Decision boundary')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b9cdab",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Again a smoother decision boundary can be obtained despite the presence of noise. Again notice that this is possible despite the fact that the capacity of the model used is probably too high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e80f3a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "### Key points\n",
    "\n",
    "In this lecture:\n",
    "\n",
    "+ We presented in detail the structure and training process for MLPs.\n",
    "+ We discussed about training MLPs and potential problems such as overfitting.\n",
    "+ We demonstrated some strategies for training MLPs and avoiding overfitting\n",
    "\n",
    "The above should allow us to solve today's problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98893132",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Resources/Further reading\n",
    "\n",
    "+ [`keras` website](https://keras.io/)\n",
    "+ I. Goodfellow, Y. Bengio and A. Courville. *Deep learning*. MIT press, 2016 (library 006.31 GOO)\n",
    "+ D.W. Patterson. *Artificial Neural Networks: Theory and Applications*. Prentice Hall, 1996 (library 006.3 PAT)\n",
    "+ K. Mehrotra, C.K. Mohan, S. Ranka. *Elements of Artificial neural networks*, MIT Press, 1997 (library 001.535 MEH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7c8865",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Solving today's problem\n",
    "\n",
    "Today's problem is a classification problem with 27 inputs and 5 classes (grade 0-4). As a first step towards it, the data should be converted to `numpy` arrays and normalised as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ecaf24",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#convert data and labels (metal grades) to numpy arrays\n",
    "dataMetal = metalDf.iloc[:,:-1].to_numpy()\n",
    "labelsMetal = metalDf.iloc[:,-1].to_numpy()\n",
    "\n",
    "#normalise data using the maximum value of each column\n",
    "dataMetal = dataMetal/dataMetal.max(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4503e5ee",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Next we can split the data into training and testing sets using a 80%-20% split and, subsequently, further split the training data into training and validation using a 80%-20% split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca00349",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "677916f8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A MLP can be created to classify the data and early stopping can be employed to avoid overfitting. The number of inputs and outputs for the network should be 27 and 5 respectively, while a single hidden layer can be used. Different numbers of hidden units, for instance 50, 100, 200 can be tested manually or automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd790cb6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fa603d1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The model error and accuracy should be evaluated for the test set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce39758",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd2811ec",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Finally, the labels of the test set can be predicted and visualised using a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d40f26",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cc503a5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Additional practice\n",
    "\n",
    "For some additional practice, in the cells below, the full iris data set is loaded and normalised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da87982",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd #import pandas module\n",
    "\n",
    "#pd.set_option('display.max_rows', None) #uncomment this line to view full dataset\n",
    "\n",
    "irisDf = pd.read_csv(\"iris_full.csv\") #read the file containing the data\n",
    "\n",
    "irisDf #visualise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0dbdc6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "irisData=irisDf[['sepal length','sepal width','petal length','petal width']].to_numpy()\n",
    "irisData = irisData/irisData.max(axis=0)\n",
    "\n",
    "irisLabels = np.zeros(150)\n",
    "irisLabels[50:100]=1\n",
    "irisLabels[100:]=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3945a5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Similar to what was shown above, the dataset can be split into training, validation and testing parts and a MLP can be trained to classify the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcafe21",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
