{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3: Classification, Evaluation, and Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you will experience a complete machine learning cycle from data preparation to deployment. You will prepare/preprocess a dataset, make a model, evaluate models to find the best fit, and deploy it to a simple web page. Our main objective is to make you try classification and evaluation methods, so we will only apply essential data preprocessing techniques but mainly focus on classification and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the **Adult** dataset from the UCI repository and more information about the data is available [here](http://archive.ics.uci.edu/ml/datasets/Adult). Since we have removed and changed the dataset for a grading purpose, use the one that we provide on ilearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains the information to check whether income exceeds $50K/yr based on census data. The datasets consist of 14 attributes and one binary class variable:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- income: >50K, <=50K\n",
    "\n",
    "- age: continuous.\n",
    "\n",
    "- workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
    "\n",
    "- fnlwgt: continuous.\n",
    "\n",
    "- education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool. education-num: continuous.\n",
    "\n",
    "- education-num: continuous.\n",
    "\n",
    "- marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
    "\n",
    "- occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
    "\n",
    "- relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
    "\n",
    "- race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
    "\n",
    "- sex: Female, Male.\n",
    "\n",
    "- capital-gain: continuous.\n",
    "\n",
    "- capital-loss: continuous.\n",
    "\n",
    "- hours-per-week: continuous.\n",
    "\n",
    "- native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we have a binary class which can be `>=50` or `<50`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "- Unlike the labs, each function you make here will be **graded**, so it is important to *strictly* follow the instruction.\n",
    "- **Import** all necessary libraries yourself whenever needed. Failure to run any code can affect your grade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Total points: 6.0 pt**\n",
    "\n",
    "0. Preparation (0.7 pt)\n",
    " - Task 1: Drop missing values (0.2 pt).\n",
    " - Task 2: Assign X and y (0.1 pt).\n",
    " - Task 3: One-hot encoding (0.1 pt).\n",
    " - Task 4: Train test split (0.1 pt).\n",
    " - Task 5: Standardization (0.2 pt).\n",
    "1. Classification (2.9 pt)\n",
    " - Task 6: Random forest (0.5 pt).\n",
    " - Task 7: SVM with diverse kernels (0.4 pt).\n",
    " - Task 8: Decision tree and Random Forest (2.0 pt).\n",
    "2. Evaluation (1.8 pt)\n",
    " - Task 9: Accuracy, Precision, Recall, F1-score (0.3 pt).\n",
    " - Task 10: AUC/AUPRC (0.3 pt).\n",
    " - Task 11: Apply them together with scikit-learn (0.4 pt).\n",
    " - Task 12: Manual implementation of performance metrics (0.8 pt).\n",
    "3. Deployment (0.6 pt)\n",
    " - Task 13: Save models into a file using pickle (0.3 pt).\n",
    " - Task 14: DASH deployment (0.3 pt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Student information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please provide your information for automatic grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUD_SUID = 'Your su account (e.g., pang1234)'\n",
    "STUD_NAME = 'First and Last name'\n",
    "STUD_EMAIL = 'student@stud.dsv.su.se'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These libraries will be frequently used throughout the homework. Do not change the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "RANDOM_STATE = 46458 #Do not change it!\n",
    "np.random.seed(RANDOM_STATE) #Do not change it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the **Adult** dataset located on ilearn, and load it here using pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult = pd.read_csv(\"datasets/adult.data\", sep=\",\", header=None, skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the line below to give the dataframe proper column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult.columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race',  'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can find out some basic information by calling *info(), head()*, and *describe()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             32561 non-null  int64 \n",
      " 1   workclass       32561 non-null  object\n",
      " 2   fnlwgt          32561 non-null  int64 \n",
      " 3   education       32561 non-null  object\n",
      " 4   education-num   32561 non-null  int64 \n",
      " 5   marital-status  32561 non-null  object\n",
      " 6   occupation      32561 non-null  object\n",
      " 7   relationship    32561 non-null  object\n",
      " 8   race            32561 non-null  object\n",
      " 9   sex             32561 non-null  object\n",
      " 10  capital-gain    32561 non-null  int64 \n",
      " 11  capital-loss    32561 non-null  int64 \n",
      " 12  hours-per-week  32561 non-null  int64 \n",
      " 13  native-country  32561 non-null  object\n",
      " 14  income          32561 non-null  object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "adult.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education-num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "       marital-status         occupation   relationship   race     sex  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country income  \n",
       "0          2174             0              40  United-States  <=50K  \n",
       "1             0             0              13  United-States  <=50K  \n",
       "2             0             0              40  United-States  <=50K  \n",
       "3             0             0              40  United-States  <=50K  \n",
       "4             0             0              40           Cuba  <=50K  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>32561.000000</td>\n",
       "      <td>3.256100e+04</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.581647</td>\n",
       "      <td>1.897784e+05</td>\n",
       "      <td>10.080679</td>\n",
       "      <td>1077.648844</td>\n",
       "      <td>87.303830</td>\n",
       "      <td>40.437456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.640433</td>\n",
       "      <td>1.055500e+05</td>\n",
       "      <td>2.572720</td>\n",
       "      <td>7385.292085</td>\n",
       "      <td>402.960219</td>\n",
       "      <td>12.347429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.228500e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.178270e+05</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>1.783560e+05</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>2.370510e+05</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.484705e+06</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>4356.000000</td>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age        fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "count  32561.000000  3.256100e+04   32561.000000  32561.000000  32561.000000   \n",
       "mean      38.581647  1.897784e+05      10.080679   1077.648844     87.303830   \n",
       "std       13.640433  1.055500e+05       2.572720   7385.292085    402.960219   \n",
       "min       17.000000  1.228500e+04       1.000000      0.000000      0.000000   \n",
       "25%       28.000000  1.178270e+05       9.000000      0.000000      0.000000   \n",
       "50%       37.000000  1.783560e+05      10.000000      0.000000      0.000000   \n",
       "75%       48.000000  2.370510e+05      12.000000      0.000000      0.000000   \n",
       "max       90.000000  1.484705e+06      16.000000  99999.000000   4356.000000   \n",
       "\n",
       "       hours-per-week  \n",
       "count    32561.000000  \n",
       "mean        40.437456  \n",
       "std         12.347429  \n",
       "min          1.000000  \n",
       "25%         40.000000  \n",
       "50%         40.000000  \n",
       "75%         45.000000  \n",
       "max         99.000000  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Drop missing values (0.2 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like there is no null data. However, if you read the description of the dataset, it says that there are missing parts represented as \"?\". You can count them by using the same technique we used for checking nulls in the previous lab. We have missing values in specific columns only, and it is about 5% of data records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Task 1-1: Count how many null values **each column** has and save it into the variable `null_values` (0.1 pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values = (adult == '?').sum()# CHANGE IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                  0\n",
      "workclass         1836\n",
      "fnlwgt               0\n",
      "education            0\n",
      "education-num        0\n",
      "marital-status       0\n",
      "occupation        1843\n",
      "relationship         0\n",
      "race                 0\n",
      "sex                  0\n",
      "capital-gain         0\n",
      "capital-loss         0\n",
      "hours-per-week       0\n",
      "native-country     583\n",
      "income               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(null_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to handle missing values, such as imputation or putting median/mean values, but we will practice the simplest way: removing the rows with missing values.\n",
    "\n",
    "- Task 1-2: Complete the function below which removes any rows with missing values (0.1 pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_missing_values(df, miss):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "      df: the dataframe (adult in our case)\n",
    "      miss: a character to represent missing value (\"?\" in our case)\n",
    "      \n",
    "    Output: the dataframe without the missing values\n",
    "\n",
    "    Step 1: Replace the value 'miss' with np.nan.\n",
    "    Step 2: Drop the nan values and store the result in data_dropped.\n",
    "    Step 3: Return data_dropped\n",
    "    \n",
    "    \"\"\"\n",
    "    df.replace(miss,np.nan,inplace=True)\n",
    "\n",
    "    data_dropped = df.dropna()\n",
    "\n",
    "    return data_dropped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply your function to our dataset `adult` and save the result to `adult_dropped`. This part should be done correctly to get the point. You need to put our dataset and the indicator for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_dropped = drop_missing_values(adult, '?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The output of the function should have the same attributes but only less number of the rows. Check how many rows are removed. Your dataset should have 30,162 rows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30162, 15)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult_dropped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Assign X and y (0.1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's split our dataset into two parts (`X` for attributes and `y` for labels) to use scikit-learn's various methods.\n",
    "- Use `adult_dropped`.\n",
    "- `X` should have all the attributes without the labels (the last column).\n",
    "- `y` should be a Pandas Series only with the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = adult_dropped.iloc[:,:-1]\n",
    "y = adult_dropped.iloc[:,-1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check the type and size here. We expect (30162, 14) for attributes (`X`) and (30162, ) for labels (`y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30162, 14), (30162,), pandas.core.frame.DataFrame, pandas.core.series.Series)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X.shape, y.shape, type(X), type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: One-hot encoding (0.1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, scikit-learn does not support categorical attributes very well even for decision tree, and that means we need to convert them into reasonal form of numeric data to fit the algorithms. There is one way called one-hot encoding, which transforms the categorical data into multiple numeric columns for each possible value. There are various ways to apply this, especially using [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) or [pandas](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html) but here we will use the pandas function to keep the dataframe structure.\n",
    "\n",
    "- Finish one_hot_encoding function which applies one-hot encoding to a given dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(df):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        df: the attributes (X in our case)\n",
    "    Output: one-hot encoded dataframe\n",
    "    \n",
    "    Step 1: Use pd.get_dummies to convert df to a one-hot-encoded form. \n",
    "            Enable an option called drop_first to remove duplication.\n",
    "    Step 2: Return the one-hot-encoded dataframe.\n",
    "    \n",
    "    * Those steps and suggested method are just for your convenience. You can use your own choice of methods.\n",
    "      However, the result should be the same as the one created with the steps above.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_onehot = pd.get_dummies(df, drop_first=True) # CHANGE IT\n",
    "    return df_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create `X_onehot` by calling `one_hot_encoding` function with `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_onehot = one_hot_encoding(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check your result by calling any methods you learned. If you successfully followed the instruction, the output (`X_onehot`) should have 96 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>workclass_Private</th>\n",
       "      <th>workclass_Self-emp-inc</th>\n",
       "      <th>workclass_Self-emp-not-inc</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_Portugal</th>\n",
       "      <th>native-country_Puerto-Rico</th>\n",
       "      <th>native-country_Scotland</th>\n",
       "      <th>native-country_South</th>\n",
       "      <th>native-country_Taiwan</th>\n",
       "      <th>native-country_Thailand</th>\n",
       "      <th>native-country_Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_United-States</th>\n",
       "      <th>native-country_Vietnam</th>\n",
       "      <th>native-country_Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>77516</td>\n",
       "      <td>13</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>83311</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>215646</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>234721</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>338409</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  fnlwgt  education-num  capital-gain  capital-loss  hours-per-week  \\\n",
       "0   39   77516             13          2174             0              40   \n",
       "1   50   83311             13             0             0              13   \n",
       "2   38  215646              9             0             0              40   \n",
       "3   53  234721              7             0             0              40   \n",
       "4   28  338409             13             0             0              40   \n",
       "\n",
       "   workclass_Local-gov  workclass_Private  workclass_Self-emp-inc  \\\n",
       "0                    0                  0                       0   \n",
       "1                    0                  0                       0   \n",
       "2                    0                  1                       0   \n",
       "3                    0                  1                       0   \n",
       "4                    0                  1                       0   \n",
       "\n",
       "   workclass_Self-emp-not-inc  ...  native-country_Portugal  \\\n",
       "0                           0  ...                        0   \n",
       "1                           1  ...                        0   \n",
       "2                           0  ...                        0   \n",
       "3                           0  ...                        0   \n",
       "4                           0  ...                        0   \n",
       "\n",
       "   native-country_Puerto-Rico  native-country_Scotland  native-country_South  \\\n",
       "0                           0                        0                     0   \n",
       "1                           0                        0                     0   \n",
       "2                           0                        0                     0   \n",
       "3                           0                        0                     0   \n",
       "4                           0                        0                     0   \n",
       "\n",
       "   native-country_Taiwan  native-country_Thailand  \\\n",
       "0                      0                        0   \n",
       "1                      0                        0   \n",
       "2                      0                        0   \n",
       "3                      0                        0   \n",
       "4                      0                        0   \n",
       "\n",
       "   native-country_Trinadad&Tobago  native-country_United-States  \\\n",
       "0                               0                             1   \n",
       "1                               0                             1   \n",
       "2                               0                             1   \n",
       "3                               0                             1   \n",
       "4                               0                             0   \n",
       "\n",
       "   native-country_Vietnam  native-country_Yugoslavia  \n",
       "0                       0                          0  \n",
       "1                       0                          0  \n",
       "2                       0                          0  \n",
       "3                       0                          0  \n",
       "4                       0                          0  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_onehot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4: Train test split (0.1 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to split our dataset further into four parts for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use scikit-learn's `train_test_split` function to divide the dataset into four parts.\n",
    "- Follow the instruction below carefully to get a point!.\n",
    "    - Use `X_onehot` and `y`.\n",
    "    - Assign 20% to a test set.\n",
    "    - Use our random state (`RANDOM_STATE`)\n",
    "    - Enable stratify option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the assigned values and write the train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_onehot,y,test_size=0.2,random_state=RANDOM_STATE,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check the type and size here. We expect 24,129 data instances in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24129, 96), (6033, 96), (24129,), (6033,))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5: Standardization (0.2 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the missing value and split X and y, we need to take care of our numeric attributes. As you can check from `describe()` function, we have numeric attributes with different mean and standard deviation values. Not all machine learning models require standardization of numeric attributes, but some do. In this homework, SVM might be the case that the standardization is required. It might be better to make standardized version when performing data preparation. \n",
    "\n",
    "- One-hot encoded data does not need to be standardized! So you need to choose the numeric columns only\n",
    "- For this task, you need to import sklearn's `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X_train, X_test, numeric):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - X_train: A split training set from Task 4\n",
    "        - X_test: A split test set from Task 5\n",
    "        - numeric: Numeric columns that should be standardized\n",
    "    Output:\n",
    "        - X_train_st: A standardized numeric attributes (ndarray)\n",
    "        - X_test_st: A standardized numeric attributes (ndarray)\n",
    "    \n",
    "    Step 1: Initialize StandardScaler into the variable 'sc'.\n",
    "    Step 2: Create X_train_numeric, X_test_numeric by selecting numeric columns from original X_train and X_test.\n",
    "            Use the input 'numeric' to choose the columns.\n",
    "    Step 3: Fit StandardScaler on X_train_numeric. You should only use the numeric columns only.\n",
    "    Step 4: Use trained StandardScaler and run transform function both on X_train_st (for the training set) \n",
    "            and X_test_st (for the test set). This job will standardize both training and test sets based on\n",
    "            the statistics of training set. You should only use numeric attributes.\n",
    "    Step 5: Return X_train_st, X_test_st\n",
    "    \n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    # Step 1\n",
    "    sc = StandardScaler()\n",
    "    \n",
    "    # Step 2\n",
    "    X_train_numeric = X_train[numeric]\n",
    "    X_test_numeric = X_test[numeric]\n",
    "    \n",
    "    # Step 3\n",
    "    sc.fit(X_train_numeric)\n",
    "    \n",
    "    # Step 4\n",
    "    # Assign two outputs of transformation function to X_train_st (for the training set) and X_test_st (for the test set)\n",
    "    X_train_st = sc.transform(X_train_numeric)\n",
    "    X_test_st = sc.transform(X_test_numeric)\n",
    "    \n",
    "    # Step 5\n",
    "    # Note that those two variable should only contain numeric attributes, not the whole ones.\n",
    "    return X_train_st, X_test_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(X_train, X_test, X_train_numeric, X_test_numeric, numeric):\n",
    "    # DO NOT CHANGE THIS FUNCTION\n",
    "    # This function is to ensure that the datasets keep the Pandas DataFrame format.\n",
    "    if X_train.shape == (0, 0): return pd.DataFrame([0]), pd.DataFrame([0])\n",
    "    \n",
    "    X_train_st_df = X_train.copy()\n",
    "    X_train_st_df[numeric] = X_train_numeric\n",
    "    X_test_st_df = X_test.copy()\n",
    "    X_test_st_df[numeric] = X_test_numeric\n",
    "    \n",
    "    return X_train_st_df, X_test_st_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find numeric columns first and assign the column names into the variable `numeric`. You can use `.info()` or `.describe()` function to find numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = list(adult.describe().columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Call `standardize` function to standardize numeric attributes. In this case, the output should only contain numeric attributes. We will merge the categorical features later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_numeric, X_test_numeric = standardize(X_train, X_test, numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check the mean and standard deviation values of the standardized dataset by running the blocks below. The dataset now should have near zero mean and one standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.36464759e-16, -4.38770225e-17,  2.26747029e-16, -2.82697595e-17,\n",
       "        3.44537694e-17,  3.03310961e-17])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_numeric.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_numeric.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately scikit-learn's StandardScaler does not return DataFrame. Run the block below to recover DataFrame and categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_st, X_test_st = merge(X_train, X_test, X_train_numeric, X_test_numeric, numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the final outcome (`X_train_st`) should have 96 columns again, where the numeric attributes have zero mean and one standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>workclass_Private</th>\n",
       "      <th>workclass_Self-emp-inc</th>\n",
       "      <th>workclass_Self-emp-not-inc</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_Portugal</th>\n",
       "      <th>native-country_Puerto-Rico</th>\n",
       "      <th>native-country_Scotland</th>\n",
       "      <th>native-country_South</th>\n",
       "      <th>native-country_Taiwan</th>\n",
       "      <th>native-country_Thailand</th>\n",
       "      <th>native-country_Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_United-States</th>\n",
       "      <th>native-country_Vietnam</th>\n",
       "      <th>native-country_Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.412900e+04</td>\n",
       "      <td>2.412900e+04</td>\n",
       "      <td>2.412900e+04</td>\n",
       "      <td>2.412900e+04</td>\n",
       "      <td>2.412900e+04</td>\n",
       "      <td>2.412900e+04</td>\n",
       "      <td>24129.000000</td>\n",
       "      <td>24129.000000</td>\n",
       "      <td>24129.000000</td>\n",
       "      <td>24129.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>24129.000000</td>\n",
       "      <td>24129.000000</td>\n",
       "      <td>24129.000000</td>\n",
       "      <td>24129.000000</td>\n",
       "      <td>24129.000000</td>\n",
       "      <td>24129.000000</td>\n",
       "      <td>24129.000000</td>\n",
       "      <td>24129.000000</td>\n",
       "      <td>24129.000000</td>\n",
       "      <td>24129.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-2.364648e-16</td>\n",
       "      <td>-4.387702e-17</td>\n",
       "      <td>2.267470e-16</td>\n",
       "      <td>-2.826976e-17</td>\n",
       "      <td>3.445377e-17</td>\n",
       "      <td>3.033110e-17</td>\n",
       "      <td>0.067844</td>\n",
       "      <td>0.739649</td>\n",
       "      <td>0.035227</td>\n",
       "      <td>0.084338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001160</td>\n",
       "      <td>0.003730</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.002445</td>\n",
       "      <td>0.001160</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.911144</td>\n",
       "      <td>0.002238</td>\n",
       "      <td>0.000497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000021e+00</td>\n",
       "      <td>1.000021e+00</td>\n",
       "      <td>1.000021e+00</td>\n",
       "      <td>1.000021e+00</td>\n",
       "      <td>1.000021e+00</td>\n",
       "      <td>1.000021e+00</td>\n",
       "      <td>0.251483</td>\n",
       "      <td>0.438835</td>\n",
       "      <td>0.184358</td>\n",
       "      <td>0.277900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034046</td>\n",
       "      <td>0.060961</td>\n",
       "      <td>0.019310</td>\n",
       "      <td>0.049389</td>\n",
       "      <td>0.034046</td>\n",
       "      <td>0.024926</td>\n",
       "      <td>0.024926</td>\n",
       "      <td>0.284541</td>\n",
       "      <td>0.047255</td>\n",
       "      <td>0.022296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.629466e+00</td>\n",
       "      <td>-1.660136e+00</td>\n",
       "      <td>-3.561849e+00</td>\n",
       "      <td>-1.467277e-01</td>\n",
       "      <td>-2.197327e-01</td>\n",
       "      <td>-3.342760e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-7.929936e-01</td>\n",
       "      <td>-6.816399e-01</td>\n",
       "      <td>-4.313815e-01</td>\n",
       "      <td>-1.467277e-01</td>\n",
       "      <td>-2.197327e-01</td>\n",
       "      <td>-8.091263e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-1.086075e-01</td>\n",
       "      <td>-1.084830e-01</td>\n",
       "      <td>-4.007307e-02</td>\n",
       "      <td>-1.467277e-01</td>\n",
       "      <td>-2.197327e-01</td>\n",
       "      <td>-8.091263e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.518215e-01</td>\n",
       "      <td>4.474502e-01</td>\n",
       "      <td>7.425438e-01</td>\n",
       "      <td>-1.467277e-01</td>\n",
       "      <td>-2.197327e-01</td>\n",
       "      <td>3.372729e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.921666e+00</td>\n",
       "      <td>1.216896e+01</td>\n",
       "      <td>2.307777e+00</td>\n",
       "      <td>1.342329e+01</td>\n",
       "      <td>9.429462e+00</td>\n",
       "      <td>4.853677e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                age        fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "count  2.412900e+04  2.412900e+04   2.412900e+04  2.412900e+04  2.412900e+04   \n",
       "mean  -2.364648e-16 -4.387702e-17   2.267470e-16 -2.826976e-17  3.445377e-17   \n",
       "std    1.000021e+00  1.000021e+00   1.000021e+00  1.000021e+00  1.000021e+00   \n",
       "min   -1.629466e+00 -1.660136e+00  -3.561849e+00 -1.467277e-01 -2.197327e-01   \n",
       "25%   -7.929936e-01 -6.816399e-01  -4.313815e-01 -1.467277e-01 -2.197327e-01   \n",
       "50%   -1.086075e-01 -1.084830e-01  -4.007307e-02 -1.467277e-01 -2.197327e-01   \n",
       "75%    6.518215e-01  4.474502e-01   7.425438e-01 -1.467277e-01 -2.197327e-01   \n",
       "max    3.921666e+00  1.216896e+01   2.307777e+00  1.342329e+01  9.429462e+00   \n",
       "\n",
       "       hours-per-week  workclass_Local-gov  workclass_Private  \\\n",
       "count    2.412900e+04         24129.000000       24129.000000   \n",
       "mean     3.033110e-17             0.067844           0.739649   \n",
       "std      1.000021e+00             0.251483           0.438835   \n",
       "min     -3.342760e+00             0.000000           0.000000   \n",
       "25%     -8.091263e-02             0.000000           0.000000   \n",
       "50%     -8.091263e-02             0.000000           1.000000   \n",
       "75%      3.372729e-01             0.000000           1.000000   \n",
       "max      4.853677e+00             1.000000           1.000000   \n",
       "\n",
       "       workclass_Self-emp-inc  workclass_Self-emp-not-inc  ...  \\\n",
       "count            24129.000000                24129.000000  ...   \n",
       "mean                 0.035227                    0.084338  ...   \n",
       "std                  0.184358                    0.277900  ...   \n",
       "min                  0.000000                    0.000000  ...   \n",
       "25%                  0.000000                    0.000000  ...   \n",
       "50%                  0.000000                    0.000000  ...   \n",
       "75%                  0.000000                    0.000000  ...   \n",
       "max                  1.000000                    1.000000  ...   \n",
       "\n",
       "       native-country_Portugal  native-country_Puerto-Rico  \\\n",
       "count             24129.000000                24129.000000   \n",
       "mean                  0.001160                    0.003730   \n",
       "std                   0.034046                    0.060961   \n",
       "min                   0.000000                    0.000000   \n",
       "25%                   0.000000                    0.000000   \n",
       "50%                   0.000000                    0.000000   \n",
       "75%                   0.000000                    0.000000   \n",
       "max                   1.000000                    1.000000   \n",
       "\n",
       "       native-country_Scotland  native-country_South  native-country_Taiwan  \\\n",
       "count             24129.000000          24129.000000           24129.000000   \n",
       "mean                  0.000373              0.002445               0.001160   \n",
       "std                   0.019310              0.049389               0.034046   \n",
       "min                   0.000000              0.000000               0.000000   \n",
       "25%                   0.000000              0.000000               0.000000   \n",
       "50%                   0.000000              0.000000               0.000000   \n",
       "75%                   0.000000              0.000000               0.000000   \n",
       "max                   1.000000              1.000000               1.000000   \n",
       "\n",
       "       native-country_Thailand  native-country_Trinadad&Tobago  \\\n",
       "count             24129.000000                    24129.000000   \n",
       "mean                  0.000622                        0.000622   \n",
       "std                   0.024926                        0.024926   \n",
       "min                   0.000000                        0.000000   \n",
       "25%                   0.000000                        0.000000   \n",
       "50%                   0.000000                        0.000000   \n",
       "75%                   0.000000                        0.000000   \n",
       "max                   1.000000                        1.000000   \n",
       "\n",
       "       native-country_United-States  native-country_Vietnam  \\\n",
       "count                  24129.000000            24129.000000   \n",
       "mean                       0.911144                0.002238   \n",
       "std                        0.284541                0.047255   \n",
       "min                        0.000000                0.000000   \n",
       "25%                        1.000000                0.000000   \n",
       "50%                        1.000000                0.000000   \n",
       "75%                        1.000000                0.000000   \n",
       "max                        1.000000                1.000000   \n",
       "\n",
       "       native-country_Yugoslavia  \n",
       "count               24129.000000  \n",
       "mean                    0.000497  \n",
       "std                     0.022296  \n",
       "min                     0.000000  \n",
       "25%                     0.000000  \n",
       "50%                     0.000000  \n",
       "75%                     0.000000  \n",
       "max                     1.000000  \n",
       "\n",
       "[8 rows x 96 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_st.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing a simple data processing, let's proceed to our main task, classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will run random forest (RF), and support vector machine (SVM) with different kernels using scikit-learn. Then we will implement score functions for decision trees and main functions for random forests to understand the concepts better. We will continue to use the pre-processed Adult dataset from the section zero (Task 1-5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6: Random forest (graded, 0.5 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you will run the random forest algorithm using scikit-learn, together with cross-validation. Detailed information about the random forest in scikit-learn can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
    "\n",
    "Your task is as follows:\n",
    " 1. Create a random forest classifier `rf` with the random state defined above (`RANDOM_STATE`). Do not specify any other parameters.\n",
    " 2. Report an average cross-validation score `rf_cross_val_score` with stratified k-fold with **cv=5**. You should report the average score, not a list of the scores. Use `X_onehot` and `y`, not the training or test set (0.2 pt). \n",
    " - ***Note that you are reporting an average cross validation score, not a list of scores.***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf_cross_val_score = cross_val_score(rf, X_onehot, y, cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run this line to check your score. Your score should be above 0.80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8499438378485088"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " 3. Run grid search `gs` with a single dictionary `grid_dict` with two keys 1) max_depth from 2 to 4 (included), and 2) min_samples_split from 2 to 5 (included) and report the best classifier into the variable `rf_best_classifier`. Set **cv=5** for grid search cross-validation. Use our training set (`X_train_st` and `y_train`) to perform the grid search. This task can take a few minutes depending on computing power (0.3 pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dict = {'max_depth' : list(range(2,5)),\n",
    "            'min_samples_split' : list(range(2,5))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gs = GridSearchCV(rf, grid_dict,cv=5).fit(X_train_st,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Report your best classifier here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_classifier = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=4, random_state=46458)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=4, random_state=46458)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=4, random_state=46458)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_best_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 7: SVM with diverse kernels (graded, 0.4 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already tried a simple SVC with the RBF kernel before. Here you will run SVM again, but with different kernels, and together with cross-validation. Detailed information about SVC in scikit-learn can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC).\n",
    "\n",
    "Your task is as follows:\n",
    "\n",
    "  1. Create a standard SVC classifier `svc` without setting any parameter.\n",
    "  2. Run grid search with a list of two parameter dictionaries, one with kernel = ['linear', 'poly', 'rbf'] and the other one with C = [1, 10, 100]. This means you have to create a list containing two different dictionaries inside. Report the best classifier into the variable `svm_best_classifier`. Set **cv=4** for grid search cross-validation. Use `X_train_st` and `y_train`. This task can take a few minutes depending on computing power (0.3 pt).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dict = {'kernel' : ['linear', 'poly', 'rbf'],\n",
    "            'C' : [1, 10, 100]}\n",
    "gs = GridSearchCV(svc, grid_dict, cv = 4).fit(X_train_st, y_train)\n",
    "svm_best_classifier = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1)\n"
     ]
    }
   ],
   "source": [
    "print(svm_best_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  3. Take the best model from grid search and report the test score to `svm_gs_score` (0.1 pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_gs_score = svm_best_classifier.score(X_train_st, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.863317999088234\n"
     ]
    }
   ],
   "source": [
    "print(svm_gs_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 8: Decision tree (2.0 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8 involves manual implementation and uses a different dataset. If you would like to complete scikit-learn related tasks using the same dataset first, please move on to Task 9 and on and come back to Task 8 later.**\n",
    "\n",
    "We will now implement a few modules for decision tree. Follow the instruction carefully so that you can return a correct result. This task is composed of two sections as follows:\n",
    "\n",
    "  - 8-1. Entropy, gini index, and information gain (0.7 pt)\n",
    "  - 8-2. Tree implementation (1.3 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First section of this task is to create three functions used to evaluate and grow the tree, which are covered in the lecture. Entropy, gini index are two main scores used for it. Information gain is the final score to choose a feature for dividing the node. Those scores are essencial for decision tree to work properly and a wrong score can lead to choosing the features that are not proper for creating a high-performing tree.\n",
    "\n",
    "- For simplicity, you will not use the **adult** dataset in this task but will use a simple **playgolf** dataset with categorical attributes.\n",
    "\n",
    "- Task 8 is a continuous task and the grade is evaluated by the result of the function. Since one function calls other functions in the task, failing to develop one function can affect the whole grade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import playgolf dataset to `playgolf`. You can find it in the homework file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "playgolf = pd.read_csv('datasets/playgolf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Outlook', 'Temp', 'Humidity', 'Windy', 'Play Golf'], dtype='object')"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playgolf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outlook</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Windy</th>\n",
       "      <th>Play Golf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rainy</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>False</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rainy</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>True</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>False</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Outlook  Temp Humidity  Windy Play Golf\n",
       "0     Rainy   Hot     High  False        No\n",
       "1     Rainy   Hot     High   True        No\n",
       "2  Overcast   Hot     High  False       Yes\n",
       "3     Sunny  Mild     High  False       Yes\n",
       "4     Sunny  Cool   Normal  False       Yes"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playgolf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Task 8-1: Create a gini index function (0.2 pt).\n",
    "  - The Gini Index is calculated by subtracting the sum of the squared probabilities of each class from one.\n",
    "  - You should double check the lecture slides and the examples below to make sure you made a correct function.\n",
    "  - You can use collections.Counter() to count labels of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(dataset):\n",
    "    \"\"\"\n",
    "    A function that calculates gini index of a given list.\n",
    "    \n",
    "    Input\n",
    "     - dataset: a list of labels.\n",
    "    Output\n",
    "     - impurity: gini index of the list.\n",
    "    \n",
    "    You do not need to keep the output name of this function, the grade only depends on the correct outputs.\n",
    "    \n",
    "    \"\"\"\n",
    "    p = 0\n",
    "    s = pd.Series(dataset)\n",
    "    for ui in s.unique():\n",
    "        p += (len(s[s==ui])/len(s))**2\n",
    "\n",
    "    \n",
    "    impurity = 1 - p # CHANGE IT\n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Your gini index is expected to have the following results:\n",
    "  - `0.5` for `[0,0,1,1]`\n",
    "  - `0.4082` for `[0,0,0,0,0,1,1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4082"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(gini([0,0,0,0,0,1,1]),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Report a gini score of the `Temp` attribute of **playgolf** to `gini_score` (0.2 pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_score = gini(playgolf.Temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print your score here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.653061224489796"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gini_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Task 8-2: Create an entropy function (0.2 pt).\n",
    "  - You should double check the lecture slides and the examples below to make sure you made a correct function. \n",
    "  - You can use collections.Counter() to count labels of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(dataset):\n",
    "    from math import log2\n",
    "    \"\"\"\n",
    "    A function that calculates entropy of a given list.\n",
    "    \n",
    "    Input\n",
    "     - dataset: a list of labels.\n",
    "    Output\n",
    "     - impurity: entropy value of the list.\n",
    "    \n",
    "    You do not need to keep the output name of this function, the grade only depends on the correct outputs.\n",
    "    \n",
    "    \"\"\"\n",
    "    info_d = 0\n",
    "    s = pd.Series(dataset)\n",
    "    for ui in s.unique():\n",
    "        pi = (len(s[s==ui])/len(s))\n",
    "        info_d += pi * log2(pi)\n",
    "\n",
    "    impurity = -info_d\n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Your entropy is expected to have the following results:\n",
    "  - `1.0` for `[0,0,1,1]`\n",
    "  - `0.8631` for `[0,0,0,0,0,1,1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([0,0,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8631"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(entropy([0,0,0,0,0,1,1]),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Report an entropy score of the `Windy` attribute of **playgolf** to `entropy_score` (0.2 pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_score = entropy(playgolf.Windy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print your score here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9852281360342516"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Task 8-3: Create an information gain function (0.3 pt). \n",
    "  - **DO NOT use entropy but only use the gini index for scores.**\n",
    "  - Check the lecture slides and the examples below to make sure you made a correct function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(labels_start, labels_split):\n",
    "    \"\"\"\n",
    "    Calculate information gain when we have an information of label distribution before and after split operation.\n",
    "    This information gain function receives two values:\n",
    "    \n",
    "    Input:\n",
    "      - labels_start: A single list of all current labels\n",
    "        e.g.) [0,0,0,0,1,1,1,1]\n",
    "      - labels_split: A list of lists representing split \n",
    "        e.g.) [ [0,0,1,1], [1,1,0,0] ]\n",
    "    \n",
    "    Then we can calculate information gain by calculating the gini index before splitting,\n",
    "    and substract (gini index of the subset * proportion of the subset) for each list after splitting from there.\n",
    "    \n",
    "    Output:\n",
    "      - info_gain: Information gain\n",
    "    \n",
    "    You do not need to keep the output name of this function, the grade only depends on the correct outputs.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    info_gain = gini(labels_start) \n",
    "\n",
    "    for lst in labels_split:\n",
    "      info_gain -= (gini(lst) * (len(lst)/len(labels_start)))\n",
    "      \n",
    "    return info_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Your information gain is expected to have the following results:\n",
    "  - `0.0` for `[0,0,0,0,1,1,1,1], [[0,0,1,1],[0,0,1,1]]`\n",
    "  - `0.5` for `[0,0,0,0,1,1,1,1], [[0,0,0,0],[1,1,1,1]]`\n",
    "  - `0.125` for `[0,0,0,0,1,1,1,1], [[0,0,1,0],[1,0,1,1]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.125"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_gain([0,0,0,0,1,1,1,1], [[0,0,1,0],[1,1,0,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we have labels before and after splitting information. Use those two values to calculate information gain and report it to `info_gain_score` using your own `information_gain` function (0.3 pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_start = [1,2,1,2,2,1,2,1,3,3,3]\n",
    "labels_split = [[3,3,3],[1,2,1,1],[2,2,1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_gain_score = information_gain(labels_start, labels_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print your score here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38842975206611574"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_gain_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have some score functions required to construct the tree. Then it is time to create the tree itself! \n",
    "\n",
    "- Note that this assignment does not aim to make whole working decision trees and random forests but the core functions to understand the algorithm.\n",
    "\n",
    " The tree works as follow:\n",
    "\n",
    "- Starting from the root, you choose few attributes to test. This does not need to be all the attributes the dataset has.\n",
    "- Iterate chosen attributes and calculate information gain, assuming you split the dataset based on each attribute.\n",
    "- Choose the column (attribute) with maximum information gain and split the dataset once again.\n",
    "- Continue growing the tree by choosing the column in the same say until we meet a closing criterion.\n",
    "\n",
    "Here, we will give you a basic `split` function used to split the dataset based on the attribute. This split function receives the attributes (`X`), the label (`y`) and one `feature` (attribute) of it, and split the whole dataset based on the categories of the selected feature and return split data subsets and label subsets. Using those split values, you are going to make few functions needed for decision trees and random forests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(X, y, attr):\n",
    "    split_attrs = []\n",
    "    split_labels = []\n",
    "    \n",
    "    for val in X[attr].unique():\n",
    "        attr_subset = []\n",
    "        label_subset = []\n",
    "        \n",
    "        for idx, row in X.iterrows():\n",
    "            if row[attr] == val and idx < len(y):\n",
    "                attr_subset.append(row)\n",
    "                label_subset.append(y[idx])\n",
    "                \n",
    "        split_attrs.append(pd.DataFrame(attr_subset))\n",
    "        split_labels.append(label_subset)\n",
    "        \n",
    "    return split_attrs, split_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the result by running the function below and also check the Windy column to understand what the function does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([     Outlook  Temp Humidity  Windy\n",
       "  0      Rainy   Hot     High  False\n",
       "  2   Overcast   Hot     High  False\n",
       "  3      Sunny  Mild     High  False\n",
       "  4      Sunny  Cool   Normal  False\n",
       "  7      Rainy  Mild     High  False\n",
       "  8      Rainy  Cool   Normal  False\n",
       "  9      Sunny  Mild   Normal  False\n",
       "  12  Overcast   Hot   Normal  False,\n",
       "       Outlook  Temp Humidity  Windy\n",
       "  1      Rainy   Hot     High   True\n",
       "  5      Sunny  Cool   Normal   True\n",
       "  6   Overcast  Cool   Normal   True\n",
       "  10     Rainy  Mild   Normal   True\n",
       "  11  Overcast  Mild     High   True\n",
       "  13     Sunny  Mild     High   True],\n",
       " [['No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes'],\n",
       "  ['No', 'No', 'Yes', 'Yes', 'Yes', 'No']])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split(playgolf.drop('Play Golf', axis=1), playgolf['Play Golf'], 'Windy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     False\n",
       "1      True\n",
       "2     False\n",
       "3     False\n",
       "4     False\n",
       "5      True\n",
       "6      True\n",
       "7     False\n",
       "8     False\n",
       "9     False\n",
       "10     True\n",
       "11     True\n",
       "12    False\n",
       "13     True\n",
       "Name: Windy, dtype: bool"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playgolf['Windy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Task 8-4: First thing we need to make is the function to choose the attributes. So we receive a dataframe and a strategy, and return a list of chosen columns. In this case we usually do it 'with replacement', meaning that the same column can be chosen twice. We can have three different options for choosing attributes (0.3 pt).\n",
    "  - \"sqrt\": use the sqrt of total column size to choose the columns\n",
    "  - integer numbers: use the received number for  \n",
    "  - \"max\": choose the same number of columns as its original size - just return the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_attributes(X, strategy):\n",
    "    from math import sqrt\n",
    "    from numpy.random import choice\n",
    "\n",
    "    \"\"\"\n",
    "    Input\n",
    "        - X: Attributes of the node.\n",
    "        - y: dataset labels.\n",
    "        - strategy: strategy for the number of attributes that the algorithm chooses.\n",
    "    Output\n",
    "        - attributes: a list of selected attributes\n",
    "\n",
    "    Step 1: Check the strategy. If the type of strategy is integer, number of attribute to choose will be that number.\n",
    "            If it's \"sqrt\", then it will be the square root of column size (if it is a floating point number, round it down), \n",
    "            if \"max\", it's the size of dataset's columns. Put appropriate value to 'num_attr'.\n",
    "    Step 2: Choose 'num_attr' column names from X.columns without allowing replacement. Use np.random.choice. \n",
    "            Assign the result to 'attributes'\n",
    "    Step 3: return 'attributes'.\n",
    "\n",
    "    \"\"\"\n",
    "    if type(strategy) is int:\n",
    "        num_attr = strategy\n",
    "    elif strategy == 'sqrt':\n",
    "        num_attr = round(sqrt(len(X.columns)))\n",
    "    elif strategy == 'max':\n",
    "        num_attr = len(X.columns)\n",
    "\n",
    "    attributes = list(X.columns[choice(len(X.columns),num_attr,replace=False)])\n",
    "        \n",
    "    return attributes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can test your method here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Humidity', 'Outlook']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_attributes(playgolf.drop('Play Golf', axis=1), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Temp', 'Humidity', 'Outlook', 'Windy']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_attributes(playgolf.drop('Play Golf', axis=1), \"max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Task 8-5: Now we have a method to choose attributes. Then now we need to make a function to iterate those selected attributes in the dataset and eventually can make a function for choosing the best feature to split, given the dataset of the node (It will be a full dataset if we run this function on the root node.). The function receives the datasets (`X`, `y`) and the list of selected attributes calculated by `selected_attributes` function (or it can also be user input), and returns the best feature among the chosen one and it's information gain. This process is one of the core processes of the decision tree (0.3 pt). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_info_gain_per_attribute(X, y, attributes):\n",
    "    \"\"\"\n",
    "    Input\n",
    "        - X: Attributes of the node.\n",
    "        - y: dataset labels.\n",
    "        - attributes: the selected attributes to test.\n",
    "    Output\n",
    "        - best_feature: The best feature in terms of information gain.\n",
    "        - best_gain: The information gain value when the dataset is split by the best feature.\n",
    "        \n",
    "    Step 1: Initialize two variables: Set best_info_gain to zero and best_attr to None.\n",
    "    Step 2: You should iterate the attributes we get as an input.\n",
    "            For each chosen attribute, 'split' the dataset using the split function we have offered.\n",
    "            This will return sets of attributes and labels. Save the split attributes and labels.\n",
    "    Step 3: Calculate information gain of the current split in the iteration. \n",
    "            Use information_gain function you created and the label information from Step 3.\n",
    "    Step 4: Compare it to the current best gain, if the new gain is higher (not higher or equal to), reset best_gain and best_feature.\n",
    "    Step 5: Return best_attr, best_info_gain.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # Step 1\n",
    "    best_info_gain = 0 # CHANGE IT\n",
    "    best_attr = None\n",
    "    \n",
    "    # Step 2 - You should create a for loop and Step 4 and 5 will run inside the loop\n",
    "    for attr in attributes:\n",
    "        As,Ls = split(X, y, attr)\n",
    "        # Step 3\n",
    "        ig = information_gain(y,Ls)\n",
    "        # Step 4\n",
    "        if ig > best_info_gain:\n",
    "            best_info_gain = ig\n",
    "            best_attr = attr\n",
    "\n",
    "    # Step 5\n",
    "    return best_attr, best_info_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we can check and you need to answer here! - which attribute is better in terms of information gain between Windy and Outlook?\n",
    "  - Answering this question is also part of Task 8-5.\n",
    "  - You should not put the string value yourself. Save the outcome of check_info_gain_per_attribute function that answers the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_attribute = check_info_gain_per_attribute(playgolf.drop('Play Golf', axis=1),playgolf['Play Golf'],['Outlook','Windy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Outlook', 0.11632653061224485)\n"
     ]
    }
   ],
   "source": [
    "print(better_attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 8-6: Now we can create a wrapper function that calls two separate function: `select_attributes`, and `check_info_gain_per_attribute`. Now, the function receives data `X`, `y`, and `strategy` and calls `select_attributes` with `strategy` to get a list of attributes. Next it calls `check_info_gain_per_attribute` with selected attributes and finally return the best attribute and the corresponding information gain (0.2 pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.2 pt\n",
    "def best_split(X, y, strategy):\n",
    "    \"\"\"\n",
    "    Input\n",
    "        - X: Attributes of the node.\n",
    "        - y: dataset labels.\n",
    "        - strategy: strategy for the number of attributes that the algorithm chooses.\n",
    "    Output\n",
    "        - best_feature: The best feature in terms of information gain.\n",
    "        - best_gain: The information gain value when the dataset is split by the best feature.\n",
    "\n",
    "    Complete the function following the instruction above\n",
    "    \"\"\"\n",
    "    attrs = select_attributes(X,strategy)\n",
    "\n",
    "    best_attr, best_info_gain = check_info_gain_per_attribute(X,y,attrs)\n",
    "    \n",
    "    return best_attr, best_info_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find the best split of the playgolf dataset with `strategy` = `sqrt`. Report the best feature and best gain to `best_attr_playgolf` and `best_gain_playgolf`.\n",
    "   - Answering the question here is part of Task 8-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_STATE)\n",
    "best_attr_playgolf, best_gain_playgolf = best_split(playgolf.drop(['Play Golf'], axis=1),playgolf['Play Golf'],'sqrt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Outlook', 0.11632653061224485)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST YOUR RESULT HERE\n",
    "best_attr_playgolf, best_gain_playgolf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 8-7: Now we have functions 1) to split the function based on one chosen feature (`split`) and 2) to choose the best feature to split (`best_split`). The next step will be to create one tree with all the information we have. Finish the function `build`. This function makes one tree of the random forest by using two previous functions. Since this function is a recursive one, it will return a complete tree, not a node (0.5 pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.5 pt\n",
    "\n",
    "def build(X, y, strategy, max_depth = 5, min_samples_leaf = 5, tol=0.00001, _depth = 0):\n",
    "    \"\"\"\n",
    "    Input\n",
    "        - X: Attributes of the data\n",
    "        - y: dataset labels\n",
    "        - strategy: strategy for the number of attributes that the algorithm chooses.\n",
    "        - max_depth: maximum allowed depth of the tree\n",
    "        - min_samples_leaf: minimum number of data instances required to continue\n",
    "        - tol: information gain tolerance value.\n",
    "        - _depth: current depth of tree starting from zero (root). only controlled by the algorithm.\n",
    "    Output\n",
    "        - node: a leaf or middle node.\n",
    "    \n",
    "    Step 0: Consider some stopping criteria. We do not continue this function if the following contidions are met:\n",
    "        1. if the current depth number is bigger than max_depth\n",
    "        2. if the current sample size is smaller than min_samples_leaf\n",
    "      Check these conditions and terminate the function if required. When terminating, return False\n",
    "    Step 1: Run the best split function to get the best attributes and the best information gain for the node.\n",
    "    Step 2: Examine the best information gain value. If it is lower than the tolerance value (tol), \n",
    "            return the node with the best information gain value. The node should be a dictionary form \n",
    "            {\"type\": \"leaf\", \"gain\": the best information gain}.\n",
    "    Step 3: If the best information gain is higher, split the dataset with the chosen best attribute.\n",
    "    Step 4: Create an empty list called \"branches\" to save all the branches of the current node.\n",
    "    Step 5: For each split attributes and labels, run this 'build' function recursively and store the result\n",
    "            to the  \"branches\" list **only if the returned value is not False (from termination)**.\n",
    "            Do not forget to increase the depth value so we can trace the max_depth.\n",
    "    Step 6: After all the recursion process is done, return the root node with its best attribute, branch information,\n",
    "            and the best information gain.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 0\n",
    "    \n",
    "    if _depth > max_depth or len(X) < min_samples_leaf:\n",
    "        return False\n",
    "        \n",
    "    # Step 1\n",
    "    best_attr, best_info_gain = best_split(X,y,strategy)\n",
    "\n",
    "    # Step 2 - Change the if condition and the return value\n",
    "    if best_info_gain < tol:\n",
    "        return {\"type\": \"leaf\", \"gain\": best_info_gain}\n",
    "    \n",
    "    # Step 3\n",
    "    split_attrs, split_labels = split(X,y,best_attr)\n",
    "    \n",
    "    # Step 4\n",
    "    branches = []\n",
    "    # Step 5 - You should create a for loop following the manual\n",
    "    for i,attrs in enumerate(split_attrs):\n",
    "        result = build(attrs,split_labels[i],strategy, max_depth=max_depth, min_samples_leaf= min_samples_leaf, _depth = _depth + 1)\n",
    "        if result:\n",
    "            branches.append(result)\n",
    "    # Step 6 - Change None values to the correct values\n",
    "    return {\n",
    "        \"type\": \"node\",\n",
    "        \"best_feature\": best_attr,\n",
    "        \"branches\": branches,\n",
    "        \"value\": best_info_gain\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bulid one tree using the whole playgolf dataset and report the tree to the variable 'single_tree'. Set the strategy to \"sqrt\", max_depth to 3, and min_samples_leaf to 2. You do not need to specify the tolerance value (0.5 pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAPTOP WORLD\\AppData\\Local\\Temp\\ipykernel_27276\\2509940099.py:14: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  s = pd.Series(dataset)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(RANDOM_STATE)\n",
    "single_tree = build(playgolf.drop(['Play Golf'], axis=1),playgolf['Play Golf'],'sqrt',3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print your result here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'node',\n",
       " 'best_feature': 'Outlook',\n",
       " 'branches': [{'type': 'node',\n",
       "   'best_feature': 'Outlook',\n",
       "   'branches': [{'type': 'leaf', 'gain': 0}],\n",
       "   'value': 0.48},\n",
       "  {'type': 'leaf', 'gain': 0},\n",
       "  {'type': 'node',\n",
       "   'best_feature': 'Outlook',\n",
       "   'branches': [{'type': 'node',\n",
       "     'best_feature': 'Temp',\n",
       "     'branches': [],\n",
       "     'value': 0.5}],\n",
       "   'value': 0.27999999999999997}],\n",
       " 'value': 0.11632653061224485}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 9: Accuracy, Precision, Recall, F1-score (0.3 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start to use our original dataset again! Remember that we have two standardized datasets (`X_train_st` and `y_train`).. You will evaluate the random forest and the support vector machine classifier with various performance measures you have learned besides accuracy, such as precision, recall, and F1-score, also using scikit-learn. Here we continue to use the same dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use standardized datasets (`X_train_st`, `y_train`...) throughout the task. To use the various score functions here, you need to convert the labels of `y_train` and `y_test` (`<=50K`, `>50K`) to numeric ones (0 or 1) since the score functions will not recognize the categorical labels. Create `y_train_numeric` and `y_test_numeric` with the converted labels (`<=50K` to 0, and `>50K` to 1). Refer to the previous labs. (0.1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtask 1: 0.1 pt\n",
    "y_train_numeric = pd.get_dummies(y_train).iloc[:,-1]\n",
    "y_test_numeric = pd.get_dummies(y_test).iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if you successfully replaced the values here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1], dtype=uint8), array([1, 0], dtype=uint8))"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_numeric.unique(), y_test_numeric.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create an instance of an SVC classifier with the **polynomial** kernel and fit the model using the training set. You can use any variable name for your SVC instance. Report precision score, recall score, and F1-score using the test set, and save it into the variable `recall_score_svc`, `precision_score_svc`, and `f1_score_svc` (0.2 pt). \n",
    "  - You can find out the information about the performance measures [here](https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics). We require you to calculate the scores using the following functions in scikit-learn: `precision_score`, `recall_score`, and `f1_score`. \n",
    "  - Use the `macro` average option - you can read more about it [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html).\n",
    "  - There is no partial point if you are correct on only some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but SVC was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(kernel='poly').fit(X_train_st.values,y_train_numeric.values)\n",
    "pred = svc.predict(X_test_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "# Subtask 2: 0.2 pt\n",
    "recall_score_svc = precision_score(y_test_numeric,pred)\n",
    "precision_score_svc = recall_score(y_test_numeric,pred)\n",
    "f1_score_svc = f1_score(y_test_numeric,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print three scores here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5892143808255659, 0.7332228666114333, 0.6533776301218162)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score_svc, recall_score_svc, f1_score_svc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 10: AUC / AUPRC (0.3 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will evaluate the random forest and the support vector machine classifier with various performance measures related to the ROC curve, such as the area under the ROC curve (AUC) and rea under the precision-recall curve (AUPRC). Use the same dataset as the ones for Task 7 (`X_train_st` and `y_train_numeric`). AUPRC and AOC score also only recognize numeric labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create an instance of a random forest classifier without setting any constraint. Don't forget to set the random state to our value `RANDOM_STATE` and fit the model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_10 = RandomForestClassifier(random_state=RANDOM_STATE).fit(X_train_st,y_train_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print the accuracy on the test set to `accuracy_rf` (0.1 pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_rf = accuracy_score(y_test_numeric, rf_10.predict(X_test_st))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Report AUC and AUPRC using the test set, and save it into the variable called *auc_rf, auprc_rf*. You can find out the information about the performance measures [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) for AUC score, and [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html) for AUPRC score (0.2 pt). \n",
    "  - AUPRC has many names, and it is supported as *average precision score* in scikit-learn. We require you to calculate the scores using the following functions: *roc_auc_score, average_precision_score*. There is no partial point if you are correct on only some of them.\n",
    "  - You should use the probability of the predictions to calculate AUC/AUPRC.\n",
    "  - Use `weighted` average option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "auc_rf = roc_auc_score(y_test_numeric, rf_10.predict(X_test_st),average='weighted')\n",
    "auprc_rf = average_precision_score(y_test_numeric, rf_10.predict(X_test_st),average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print your scores here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8481684070943146, 0.7760772732656024, 0.5487764577694629)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(accuracy_rf, auc_rf, auprc_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 11: Apply them together with scikit-learn (0.4 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you will try to apply the grid search using the performance measures you have tried on Task 5 and Task 6, and pick the best performing model in terms of specific performance measures.\n",
    "\n",
    "Our dataset is imbalanced, meaning that the healthy patient is dominant. Therefore, we can expect that the best model can be different, and we may also need to use AUPRC to get the most suitable model. \n",
    "\n",
    "Your task is as follows:\n",
    "\n",
    "1. Use the same dataset as the ones for Task 7 (`X_train_st` and `y_train_numeric`). AUPRC and AOC score also only recognize numeric labels.\n",
    "2. Create an instance of a kNN classifier without setting any constraint. \n",
    "3. Run grid search with a dictionary - here you need to search the number of neighbors for kNN from 1 to 10 (included), and use two different scoring measures: AUC and recall. **Do not specify any average option.**\n",
    "4. Put the best classifiers into the respective variable called *auc_best_classifier* and *recall_best_classifier*. Set cv=5 for grid search cross-validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you complete the method, you can run the following line to check whether your functions are correct or not. Note that we will evaluate your functions with different data, so be careful to implement them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_best_classifier = GridSearchCV(knn,{'n_neighbors' : range(1,11)}, scoring='roc_auc', cv= 5).fit(X_train_st,y_train_numeric).best_estimator_\n",
    "recall_best_classifier = GridSearchCV(knn,{'n_neighbors' :range(1,11)}, scoring='recall', cv= 5).fit(X_train_st,y_train_numeric).best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your scores here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(KNeighborsClassifier(n_neighbors=10), KNeighborsClassifier())"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(auc_best_classifier, recall_best_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 12: Task 5 implementation (0.8 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task requires you to implement the following performance measures:\n",
    " - Accuracy (0.2 pt)\n",
    " - Precision (0.2 pt)\n",
    " - Recall (0.2 pt)\n",
    " - F1-score (0.2 pt)\n",
    " \n",
    "All inputs will be the NumPy arrays, so you can use any NumPy array methods to calculate the scores.\n",
    "  - Scikit-learn methods are not allowed\n",
    "  - Looping the list is not allowed\n",
    "  - Copying materials from internet is completely prohibited\n",
    "\n",
    "For Precision, Recall, and F1, we want you to calculate `macro` average. Read more about it [here](https://tomaxent.com/2018/04/27/Micro-and-Macro-average-of-Precision-Recall-and-F-Score/). You can always validate your method by comparing the result to the one from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_manual(truth, predicted):\n",
    "    return (truth==predicted).sum()/len(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_manual(truth, predicted):\n",
    "    return ((truth==1)&(predicted==1)).sum()/(predicted==1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_manual(truth, predicted):\n",
    "    return ((truth==1)&(predicted==1)).sum()/(truth==1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_manual(truth, predicted):\n",
    "    return (2*precision_manual(truth, predicted)*recall_manual(truth, predicted))/(precision_manual(truth, predicted)+recall_manual(truth, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the results of your four function on two arrays (`truth`, `predicted`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth     = np.array([1,0,0,0,1,1,1,0,0,1,1,0,0,1,1])\n",
    "predicted = np.array([1,0,1,0,1,1,0,1,1,0,1,1,1,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_manual = accuracy_manual(truth,predicted)\n",
    "precision_score_manual = precision_manual(truth,predicted)\n",
    "recall_score_manual = recall_manual(truth,predicted)\n",
    "f1_score_manual = f1_score_manual(truth,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show your results here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4666666666666667, 0.5, 0.625, 0.5555555555555556)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(accuracy_score_manual, precision_score_manual, recall_score_manual, f1_score_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 13: Save models into a file using pickle (0.3 pt)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you need to pick the best model using cross-validation and deploy it as a pickle file. For this task, we will use the diabetes data that we used for Homework 1. This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage. You can find it in the homework folder.\n",
    "\n",
    "- Load the dataset into `diabetes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv(\"datasets/diabetes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split the dataset into two parts: attributes (`X`) and labels (Outcome, `y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = diabetes.iloc[:,:-1]\n",
    "y = diabetes.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is as follows:\n",
    "\n",
    "1. Create an instance of an SVC classifier without setting any constraint.\n",
    "2. Run grid search with a list of two dictionaries. In the first dictionary, you should examine 'poly' kernel, with degree = [2, 3, 4]. In the second dictionary, you should test two kernels ['linear', 'rbf'] with a list of C values [10, 100]. Use **AUPRC** as its scoring measure. Set cv=5 for grid search cross-validation. Since grid search uses stratified k-fold inside, you should put the complete dataset.\n",
    "3. Save the best classifier into the variable called `svm_best_classifier_2` and save the trained model into `model_diabetes.pickle` using pickle. When saving your model, do not specify any folder.\n",
    " - **Do not use your own specific name for the model!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "\n",
    "params = [{'kernel': ['poly'], 'degree': [2,3,4]},\n",
    "                    {'kernel': ['linear','rbf'], 'C': [10, 100]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_best_classifier_2 = GridSearchCV(svc, params, scoring='average_precision', cv = 5).fit(X,y).best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show your classifier here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=10)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_best_classifier_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(svm_best_classifier_2, open('model_diabetes.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 14: DASH deployment (0.3 pt)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will run DASH application by using the model `model_diabetes.pickle` you exported with the project files in the `webplatform_dash` folder. Locate the model in the same folder with this jupeter notebook, and go into `webplatform_dash` folder. There you can run your own DASH application as you learned from Lab 5. Note that you should **not** move the model file into `webplatform_dash`.\n",
    "\n",
    "- Submit a screenshot with your model file into one zip file in a separate submission form for your DASH project. For the details of the DASH deployment, check out Lab 5."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "346812b7c4f63123021a0011a7ad9c451703c57cd6c6827f3f726952f0a6ac21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
