{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for C:\\Users\\LAPTOP WORLD\\.keras-ocr\\crnn_kurapan.h5\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(1, 31, 200, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " permute (Permute)              (1, 200, 31, 1)      0           ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (1, 200, 31, 1)      0           ['permute[0][0]']                \n",
      "                                                                                                  \n",
      " conv_1 (Conv2D)                (1, 200, 31, 64)     640         ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " conv_2 (Conv2D)                (1, 200, 31, 128)    73856       ['conv_1[0][0]']                 \n",
      "                                                                                                  \n",
      " conv_3 (Conv2D)                (1, 200, 31, 256)    295168      ['conv_2[0][0]']                 \n",
      "                                                                                                  \n",
      " bn_3 (BatchNormalization)      (1, 200, 31, 256)    1024        ['conv_3[0][0]']                 \n",
      "                                                                                                  \n",
      " maxpool_3 (MaxPooling2D)       (1, 100, 15, 256)    0           ['bn_3[0][0]']                   \n",
      "                                                                                                  \n",
      " conv_4 (Conv2D)                (1, 100, 15, 256)    590080      ['maxpool_3[0][0]']              \n",
      "                                                                                                  \n",
      " conv_5 (Conv2D)                (1, 100, 15, 512)    1180160     ['conv_4[0][0]']                 \n",
      "                                                                                                  \n",
      " bn_5 (BatchNormalization)      (1, 100, 15, 512)    2048        ['conv_5[0][0]']                 \n",
      "                                                                                                  \n",
      " maxpool_5 (MaxPooling2D)       (1, 50, 7, 512)      0           ['bn_5[0][0]']                   \n",
      "                                                                                                  \n",
      " conv_6 (Conv2D)                (1, 50, 7, 512)      2359808     ['maxpool_5[0][0]']              \n",
      "                                                                                                  \n",
      " conv_7 (Conv2D)                (1, 50, 7, 512)      2359808     ['conv_6[0][0]']                 \n",
      "                                                                                                  \n",
      " bn_7 (BatchNormalization)      (1, 50, 7, 512)      2048        ['conv_7[0][0]']                 \n",
      "                                                                                                  \n",
      " model (Functional)             (None, 6)            934902      ['bn_7[0][0]']                   \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (1, 50, 7, 512)      0           ['bn_7[0][0]',                   \n",
      "                                                                  'model[0][0]']                  \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (1, 50, 3584)        0           ['lambda_1[0][0]']               \n",
      "                                                                                                  \n",
      " fc_9 (Dense)                   (1, 50, 128)         458880      ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_10 (LSTM)                 (1, 50, 128)         131584      ['fc_9[0][0]']                   \n",
      "                                                                                                  \n",
      " lstm_10_back (LSTM)            (1, 50, 128)         131584      ['fc_9[0][0]']                   \n",
      "                                                                                                  \n",
      " add (Add)                      (1, 50, 128)         0           ['lstm_10[0][0]',                \n",
      "                                                                  'lstm_10_back[0][0]']           \n",
      "                                                                                                  \n",
      " lstm_11 (LSTM)                 (1, 50, 128)         131584      ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " lstm_11_back (LSTM)            (1, 50, 128)         131584      ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (1, 50, 256)         0           ['lstm_11[0][0]',                \n",
      "                                                                  'lstm_11_back[0][0]']           \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (1, 50, 256)         0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " fc_12 (Dense)                  (1, 50, 37)          9509        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_2 (Lambda)              (1, 48, 37)          0           ['fc_12[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,794,267\n",
      "Trainable params: 8,791,707\n",
      "Non-trainable params: 2,560\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras_ocr_tflite import build_model, CTCDecoder, model, DEFAULT_ALPHABET\n",
    "import cv2\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 125ms/step\n",
      "kibgs\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread(r\"C:\\python\\projects\\Upwork-Projects\\Sara RCNN\\rego_images\\data\\data - 20230403_141503-KP369-a7748e94-915f-48b6-91d1-8a56f3641129-cutout.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "image = cv2.resize(image, (200, 31))\n",
    "image = np.expand_dims(image, axis=-1)\n",
    "image = np.expand_dims(image, axis=0)\n",
    "image = image / 255.0  # Normalize image\n",
    "\n",
    "prediction = model.predict(image)\n",
    "decoded_text = CTCDecoder()(prediction)\n",
    "text = ''.join([DEFAULT_ALPHABET[i] for i in decoded_text[0] if i != -1])\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    (h, w) = img.shape\n",
    "    \n",
    "    final_img = np.ones([64, 256])*255 # blank white image\n",
    "    \n",
    "    # crop\n",
    "    if w > 256:\n",
    "        img = img[:, :256]\n",
    "        \n",
    "    if h > 64:\n",
    "        img = img[:64, :]\n",
    "    \n",
    "    \n",
    "    final_img[:h, :w] = img\n",
    "    return cv2.rotate(final_img, cv2.ROTATE_90_CLOCKWISE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(r\"C:\\python\\projects\\Upwork-Projects\\Sara RCNN\\rego_images\\data\\data - 20230403_141503-KP369-a7748e94-915f-48b6-91d1-8a56f3641129-cutout.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "image = preprocess(image)\n",
    "image = image / 255.  # Normalize the image\n",
    "image = np.expand_dims(image, axis=0)  # Add batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot set tensor: Dimension mismatch. Got 3 but expected 4 for input 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\python\\projects\\Upwork-Projects\\Sara RCNN\\rego_images\\predicting.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m interpreter\u001b[39m.\u001b[39mget_input_details()[\u001b[39m0\u001b[39m]  \u001b[39m# Model has single input.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Set input tensor\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m interpreter\u001b[39m.\u001b[39;49mset_tensor(\u001b[39minput\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mindex\u001b[39;49m\u001b[39m'\u001b[39;49m], image\u001b[39m.\u001b[39;49mastype(np\u001b[39m.\u001b[39;49mfloat32))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Run the interpreter\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m interpreter\u001b[39m.\u001b[39minvoke()\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:697\u001b[0m, in \u001b[0;36mInterpreter.set_tensor\u001b[1;34m(self, tensor_index, value)\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_tensor\u001b[39m(\u001b[39mself\u001b[39m, tensor_index, value):\n\u001b[0;32m    682\u001b[0m   \u001b[39m\"\"\"Sets the value of the input tensor.\u001b[39;00m\n\u001b[0;32m    683\u001b[0m \n\u001b[0;32m    684\u001b[0m \u001b[39m  Note this copies data in `value`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[39m    ValueError: If the interpreter could not set the tensor.\u001b[39;00m\n\u001b[0;32m    696\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 697\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpreter\u001b[39m.\u001b[39;49mSetTensor(tensor_index, value)\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot set tensor: Dimension mismatch. Got 3 but expected 4 for input 0."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path='2.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "output = interpreter.get_output_details()[0]  # Model has single output.\n",
    "input = interpreter.get_input_details()[0]  # Model has single input.\n",
    "\n",
    "# Set input tensor\n",
    "interpreter.set_tensor(input['index'], image.astype(np.float32))\n",
    "\n",
    "# Run the interpreter\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get output tensor\n",
    "output_data = interpreter.get_tensor(output[0]['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'serving_default_input:0',\n",
       " 'index': 0,\n",
       " 'shape': array([  1,  31, 200,   1]),\n",
       " 'shape_signature': array([  1,  31, 200,   1]),\n",
       " 'dtype': numpy.float32,\n",
       " 'quantization': (0.0, 0),\n",
       " 'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "  'zero_points': array([], dtype=int32),\n",
       "  'quantized_dimension': 0},\n",
       " 'sparsity_parameters': {}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Interpreter' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\python\\projects\\Upwork-Projects\\Sara RCNN\\rego_images\\predicting.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(image, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)  \u001b[39m# Add batch dimension\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Make a prediction using the loaded model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m predictions \u001b[39m=\u001b[39m interpreter\u001b[39m.\u001b[39;49mpredict(image)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Interpreter' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "# Assuming you have an image stored in the variable \"image\"\n",
    "image = cv2.imread(r\"C:\\python\\projects\\Upwork-Projects\\Sara RCNN\\rego_images\\data\\data - 20230403_141503-KP369-a7748e94-915f-48b6-91d1-8a56f3641129-cutout.jpg\", cv2.IMREAD_ANYCOLOR)\n",
    "image = preprocess(image)\n",
    "image = image / 255.  # Normalize the image\n",
    "image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "# Make a prediction using the loaded model\n",
    "predictions = interpreter.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d875b802a74c4c92c8f3071372b991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessor_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\LAPTOP WORLD\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e818b2cd3344406b400e8b975ff4bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983a3ccf8b5e4c528e7e75f41558ca46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce2b4ea25584542ad964bcd12052073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8985cba748451a880c73799a04c990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e23bfdb52774470b362dace5b79b7c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/4.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f2233702214561ac603b722c1ba87b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.weight', 'encoder.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78dee0c0e33d4cff96e2959be5f016fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation\\utils.py:1346: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "# load image from the IAM dataset\n",
    "url = \"https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "generated_ids = model.generate(pixel_values)\n",
    "\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation\\utils.py:1346: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "images = []\n",
    "pred_labels = []\n",
    "data_path = r'C:\\python\\projects\\Upwork-Projects\\Sara RCNN\\rego_images\\data'\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "for img in os.listdir(data_path):\n",
    "\n",
    "    images.append(img)\n",
    "\n",
    "    image = Image.open(r\"C:\\python\\projects\\Upwork-Projects\\Sara RCNN\\rego_images\\data\\data - 20230403_141503-KP369-a7748e94-915f-48b6-91d1-8a56f3641129-cutout.jpg\").convert(\"RGB\")\n",
    "\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    pred_labels.append(generated_text)\n",
    "\n",
    "    print('-',end='')\n",
    "\n",
    "df = pd.DataFrame({'images': images, 'predicted label': pred_labels})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "# import requests\n",
    "from PIL import Image\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "# load image from the IAM dataset\n",
    "\n",
    "image = Image.open(r\"C:\\python\\projects\\Upwork-Projects\\Sara RCNN\\rego_images\\data\\data - 20230403_141503-KP369-a7748e94-915f-48b6-91d1-8a56f3641129-cutout.jpg\").convert(\"RGB\")\n",
    "\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "generated_ids = model.generate(pixel_values)\n",
    "\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KP369'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.weight', 'encoder.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "InvalidSchema",
     "evalue": "No connection adapters were found for 'C:\\\\python\\\\projects\\\\Upwork-Projects\\\\Sara RCNN\\\\rego_images\\\\data\\\\data - 20230403_141503-KP369-a7748e94-915f-48b6-91d1-8a56f3641129-cutout.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidSchema\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\python\\projects\\Upwork-Projects\\Sara RCNN\\rego_images\\predicting.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#X21sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# load image from the IAM dataset\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#X21sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mpython\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mprojects\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mUpwork-Projects\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mSara RCNN\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mrego_images\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mdata - 20230403_141503-KP369-a7748e94-915f-48b6-91d1-8a56f3641129-cutout.jpg\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#X21sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(requests\u001b[39m.\u001b[39;49mget(url, stream\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39mraw)\u001b[39m.\u001b[39mconvert(\u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#X21sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m pixel_values \u001b[39m=\u001b[39m processor(image, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mpixel_values\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#X21sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mindustry, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m Mr. Brown commented icily. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m Let us have a\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39m\u001b[39mget\u001b[39m\u001b[39m\"\u001b[39m, url, params\u001b[39m=\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    585\u001b[0m }\n\u001b[0;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\sessions.py:695\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    692\u001b[0m hooks \u001b[39m=\u001b[39m request\u001b[39m.\u001b[39mhooks\n\u001b[0;32m    694\u001b[0m \u001b[39m# Get the appropriate adapter to use\u001b[39;00m\n\u001b[1;32m--> 695\u001b[0m adapter \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_adapter(url\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49murl)\n\u001b[0;32m    697\u001b[0m \u001b[39m# Start time (approximately) of the request\u001b[39;00m\n\u001b[0;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\sessions.py:792\u001b[0m, in \u001b[0;36mSession.get_adapter\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[39mreturn\u001b[39;00m adapter\n\u001b[0;32m    791\u001b[0m \u001b[39m# Nothing matches :-/\u001b[39;00m\n\u001b[1;32m--> 792\u001b[0m \u001b[39mraise\u001b[39;00m InvalidSchema(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo connection adapters were found for \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mInvalidSchema\u001b[0m: No connection adapters were found for 'C:\\\\python\\\\projects\\\\Upwork-Projects\\\\Sara RCNN\\\\rego_images\\\\data\\\\data - 20230403_141503-KP369-a7748e94-915f-48b6-91d1-8a56f3641129-cutout.jpg'"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    TrOCRConfig,\n",
    "    TrOCRProcessor,\n",
    "    TrOCRForCausalLM,\n",
    "    ViTConfig,\n",
    "    ViTModel,\n",
    "    VisionEncoderDecoderModel,\n",
    ")\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "# TrOCR is a decoder model and should be used within a VisionEncoderDecoderModel\n",
    "# init vision2text model with random weights\n",
    "encoder = ViTModel(ViTConfig())\n",
    "decoder = TrOCRForCausalLM(TrOCRConfig())\n",
    "model = VisionEncoderDecoderModel(encoder=encoder, decoder=decoder)\n",
    "\n",
    "# If you want to start from the pretrained model, load the checkpoint with `VisionEncoderDecoderModel`\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "# load image from the IAM dataset\n",
    "url = r\"C:\\python\\projects\\Upwork-Projects\\Sara RCNN\\rego_images\\data\\data - 20230403_141503-KP369-a7748e94-915f-48b6-91d1-8a56f3641129-cutout.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "text = \"industry, ' Mr. Brown commented icily. ' Let us have a\"\n",
    "\n",
    "# training\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "labels = processor.tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "outputs = model(pixel_values, labels=labels)\n",
    "loss = outputs.loss\n",
    "round(loss.item(), 2)\n",
    "\n",
    "# inference\n",
    "generated_ids = model.generate(pixel_values)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
