{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for C:\\Users\\LAPTOP WORLD\\.keras-ocr\\crnn_kurapan.h5\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(1, 31, 200, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " permute (Permute)              (1, 200, 31, 1)      0           ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (1, 200, 31, 1)      0           ['permute[0][0]']                \n",
      "                                                                                                  \n",
      " conv_1 (Conv2D)                (1, 200, 31, 64)     640         ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " conv_2 (Conv2D)                (1, 200, 31, 128)    73856       ['conv_1[0][0]']                 \n",
      "                                                                                                  \n",
      " conv_3 (Conv2D)                (1, 200, 31, 256)    295168      ['conv_2[0][0]']                 \n",
      "                                                                                                  \n",
      " bn_3 (BatchNormalization)      (1, 200, 31, 256)    1024        ['conv_3[0][0]']                 \n",
      "                                                                                                  \n",
      " maxpool_3 (MaxPooling2D)       (1, 100, 15, 256)    0           ['bn_3[0][0]']                   \n",
      "                                                                                                  \n",
      " conv_4 (Conv2D)                (1, 100, 15, 256)    590080      ['maxpool_3[0][0]']              \n",
      "                                                                                                  \n",
      " conv_5 (Conv2D)                (1, 100, 15, 512)    1180160     ['conv_4[0][0]']                 \n",
      "                                                                                                  \n",
      " bn_5 (BatchNormalization)      (1, 100, 15, 512)    2048        ['conv_5[0][0]']                 \n",
      "                                                                                                  \n",
      " maxpool_5 (MaxPooling2D)       (1, 50, 7, 512)      0           ['bn_5[0][0]']                   \n",
      "                                                                                                  \n",
      " conv_6 (Conv2D)                (1, 50, 7, 512)      2359808     ['maxpool_5[0][0]']              \n",
      "                                                                                                  \n",
      " conv_7 (Conv2D)                (1, 50, 7, 512)      2359808     ['conv_6[0][0]']                 \n",
      "                                                                                                  \n",
      " bn_7 (BatchNormalization)      (1, 50, 7, 512)      2048        ['conv_7[0][0]']                 \n",
      "                                                                                                  \n",
      " model (Functional)             (None, 6)            934902      ['bn_7[0][0]']                   \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (1, 50, 7, 512)      0           ['bn_7[0][0]',                   \n",
      "                                                                  'model[0][0]']                  \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (1, 50, 3584)        0           ['lambda_1[0][0]']               \n",
      "                                                                                                  \n",
      " fc_9 (Dense)                   (1, 50, 128)         458880      ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_10 (LSTM)                 (1, 50, 128)         131584      ['fc_9[0][0]']                   \n",
      "                                                                                                  \n",
      " lstm_10_back (LSTM)            (1, 50, 128)         131584      ['fc_9[0][0]']                   \n",
      "                                                                                                  \n",
      " add (Add)                      (1, 50, 128)         0           ['lstm_10[0][0]',                \n",
      "                                                                  'lstm_10_back[0][0]']           \n",
      "                                                                                                  \n",
      " lstm_11 (LSTM)                 (1, 50, 128)         131584      ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " lstm_11_back (LSTM)            (1, 50, 128)         131584      ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (1, 50, 256)         0           ['lstm_11[0][0]',                \n",
      "                                                                  'lstm_11_back[0][0]']           \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (1, 50, 256)         0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " fc_12 (Dense)                  (1, 50, 37)          9509        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_2 (Lambda)              (1, 48, 37)          0           ['fc_12[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8,794,267\n",
      "Trainable params: 8,791,707\n",
      "Non-trainable params: 2,560\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras_ocr_tflite import build_model, CTCDecoder, model, DEFAULT_ALPHABET\n",
    "import cv2\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 125ms/step\n",
      "kibgs\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread(r\"C:\\python\\projects\\Upwork-Projects\\Sara RCNN\\rego_images\\data\\data - 20230403_141503-KP369-a7748e94-915f-48b6-91d1-8a56f3641129-cutout.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "image = cv2.resize(image, (200, 31))\n",
    "image = np.expand_dims(image, axis=-1)\n",
    "image = np.expand_dims(image, axis=0)\n",
    "image = image / 255.0  # Normalize image\n",
    "\n",
    "prediction = model.predict(image)\n",
    "decoded_text = CTCDecoder()(prediction)\n",
    "text = ''.join([DEFAULT_ALPHABET[i] for i in decoded_text[0] if i != -1])\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    (h, w) = img.shape\n",
    "    \n",
    "    final_img = np.ones([64, 256])*255 # blank white image\n",
    "    \n",
    "    # crop\n",
    "    if w > 256:\n",
    "        img = img[:, :256]\n",
    "        \n",
    "    if h > 64:\n",
    "        img = img[:64, :]\n",
    "    \n",
    "    \n",
    "    final_img[:h, :w] = img\n",
    "    return cv2.rotate(final_img, cv2.ROTATE_90_CLOCKWISE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(r\"C:\\python\\projects\\Upwork-Projects\\Sara RCNN\\rego_images\\data\\data - 20230403_141503-KP369-a7748e94-915f-48b6-91d1-8a56f3641129-cutout.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "image = preprocess(image)\n",
    "image = image / 255.  # Normalize the image\n",
    "image = np.expand_dims(image, axis=0)  # Add batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot set tensor: Dimension mismatch. Got 3 but expected 4 for input 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\python\\projects\\Upwork-Projects\\Sara RCNN\\rego_images\\predicting.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m interpreter\u001b[39m.\u001b[39mget_input_details()[\u001b[39m0\u001b[39m]  \u001b[39m# Model has single input.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Set input tensor\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m interpreter\u001b[39m.\u001b[39;49mset_tensor(\u001b[39minput\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mindex\u001b[39;49m\u001b[39m'\u001b[39;49m], image\u001b[39m.\u001b[39;49mastype(np\u001b[39m.\u001b[39;49mfloat32))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Run the interpreter\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m interpreter\u001b[39m.\u001b[39minvoke()\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:697\u001b[0m, in \u001b[0;36mInterpreter.set_tensor\u001b[1;34m(self, tensor_index, value)\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_tensor\u001b[39m(\u001b[39mself\u001b[39m, tensor_index, value):\n\u001b[0;32m    682\u001b[0m   \u001b[39m\"\"\"Sets the value of the input tensor.\u001b[39;00m\n\u001b[0;32m    683\u001b[0m \n\u001b[0;32m    684\u001b[0m \u001b[39m  Note this copies data in `value`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[39m    ValueError: If the interpreter could not set the tensor.\u001b[39;00m\n\u001b[0;32m    696\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 697\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpreter\u001b[39m.\u001b[39;49mSetTensor(tensor_index, value)\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot set tensor: Dimension mismatch. Got 3 but expected 4 for input 0."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path='2.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "output = interpreter.get_output_details()[0]  # Model has single output.\n",
    "input = interpreter.get_input_details()[0]  # Model has single input.\n",
    "\n",
    "# Set input tensor\n",
    "interpreter.set_tensor(input['index'], image.astype(np.float32))\n",
    "\n",
    "# Run the interpreter\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get output tensor\n",
    "output_data = interpreter.get_tensor(output[0]['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'serving_default_input:0',\n",
       " 'index': 0,\n",
       " 'shape': array([  1,  31, 200,   1]),\n",
       " 'shape_signature': array([  1,  31, 200,   1]),\n",
       " 'dtype': numpy.float32,\n",
       " 'quantization': (0.0, 0),\n",
       " 'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "  'zero_points': array([], dtype=int32),\n",
       "  'quantized_dimension': 0},\n",
       " 'sparsity_parameters': {}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Interpreter' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\python\\projects\\Upwork-Projects\\Sara RCNN\\rego_images\\predicting.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(image, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)  \u001b[39m# Add batch dimension\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Make a prediction using the loaded model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m predictions \u001b[39m=\u001b[39m interpreter\u001b[39m.\u001b[39;49mpredict(image)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Interpreter' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "# Assuming you have an image stored in the variable \"image\"\n",
    "image = cv2.imread(r\"C:\\python\\projects\\Upwork-Projects\\Sara RCNN\\rego_images\\data\\data - 20230403_141503-KP369-a7748e94-915f-48b6-91d1-8a56f3641129-cutout.jpg\", cv2.IMREAD_ANYCOLOR)\n",
    "image = preprocess(image)\n",
    "image = image / 255.  # Normalize the image\n",
    "image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "# Make a prediction using the loaded model\n",
    "predictions = interpreter.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d875b802a74c4c92c8f3071372b991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessor_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\LAPTOP WORLD\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e818b2cd3344406b400e8b975ff4bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983a3ccf8b5e4c528e7e75f41558ca46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce2b4ea25584542ad964bcd12052073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8985cba748451a880c73799a04c990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e23bfdb52774470b362dace5b79b7c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/4.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f2233702214561ac603b722c1ba87b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.weight', 'encoder.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78dee0c0e33d4cff96e2959be5f016fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation\\utils.py:1346: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "# load image from the IAM dataset\n",
    "url = \"https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "generated_ids = model.generate(pixel_values)\n",
    "\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation\\utils.py:1346: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data - 20230403_094217-6172f02f-75df-4e7c-b1dd...</td>\n",
       "      <td>turners.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data - 20230403_094217-96b2f11d-9f00-434c-a838...</td>\n",
       "      <td>votes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data - 20230403_094217-a33648aa-fda3-40e9-bdf1...</td>\n",
       "      <td>tombers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data - 20230403_094217-ae807121-7d6a-43b2-8193...</td>\n",
       "      <td>to raise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data - 20230403_094217-bbb7f34d-860e-4d1a-aa1d...</td>\n",
       "      <td>outcome</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              images predicted label\n",
       "0  data - 20230403_094217-6172f02f-75df-4e7c-b1dd...        turners.\n",
       "1  data - 20230403_094217-96b2f11d-9f00-434c-a838...          votes.\n",
       "2  data - 20230403_094217-a33648aa-fda3-40e9-bdf1...        tombers.\n",
       "3  data - 20230403_094217-ae807121-7d6a-43b2-8193...        to raise\n",
       "4  data - 20230403_094217-bbb7f34d-860e-4d1a-aa1d...         outcome"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "images = []\n",
    "pred_labels = []\n",
    "data_path = r'C:\\python\\projects\\Upwork-Projects\\Sara RCNN\\rego_images\\data'\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "for img in os.listdir(data_path):\n",
    "\n",
    "    images.append(img)\n",
    "\n",
    "    image = Image.open(os.path.join(data_path,img)).convert(\"RGB\")\n",
    "\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    pred_labels.append(generated_text)\n",
    "\n",
    "    print('-',end='')\n",
    "\n",
    "df = pd.DataFrame({'images': images, 'predicted label': pred_labels})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json not found in HuggingFace Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab0dbb88e9e467a8c86e52ae19ec459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"ocr_model_v1\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 50, 1) dtype=float32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\python\\projects\\Upwork-Projects\\Sara RCNN\\rego_images\\predicting.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#X30sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# 5. Transpose the image because we want the time\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#X30sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# dimension to correspond to the width of the image.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#X30sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m img \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtranspose(img, perm\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#X30sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m generated_text \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(img)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#X30sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m pred_labels\u001b[39m.\u001b[39mappend(generated_text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#X30sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m,end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\LAPTOP~1\\AppData\\Local\\Temp\\__autograph_generated_file1fzj0gfi.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"ocr_model_v1\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 50, 1) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import from_pretrained_keras\n",
    "import tensorflow as tf\n",
    "\n",
    "model = from_pretrained_keras(\"keras-io/ocr-for-captcha\")\n",
    "\n",
    "\n",
    "for img in os.listdir(data_path):\n",
    "\n",
    "    images.append(img)\n",
    "\n",
    "    # 1. Read image\n",
    "    img = tf.io.read_file(os.path.join(data_path,img))\n",
    "    # 2. Decode and convert to grayscale\n",
    "    img = tf.io.decode_png(img, channels=1)\n",
    "    # 3. Convert to float32 in [0, 1] range\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # 4. Resize to the desired size\n",
    "    img = tf.image.resize(img, [50, 200])\n",
    "    # 5. Transpose the image because we want the time\n",
    "    # dimension to correspond to the width of the image.\n",
    "    img = tf.transpose(img, perm=[1, 0, 2])\n",
    "\n",
    "    generated_text = model.predict(img)\n",
    "\n",
    "    pred_labels.append(generated_text)\n",
    "\n",
    "    print('-',end='')\n",
    "\n",
    "df = pd.DataFrame({'images': images, 'predicted label': pred_labels})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "keras-io/ocr-for-captcha does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/keras-io/ocr-for-captcha/main' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:259\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 259\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[0;32m    260\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/keras-io/ocr-for-captcha/resolve/main/preprocessor_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mEntryNotFoundError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\utils\\hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    416\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 417\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    418\u001b[0m         path_or_repo_id,\n\u001b[0;32m    419\u001b[0m         filename,\n\u001b[0;32m    420\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    421\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[0;32m    422\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    423\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    424\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    425\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    426\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    427\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    428\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    429\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    430\u001b[0m     )\n\u001b[0;32m    432\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:1195\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[0;32m   1196\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m   1197\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[0;32m   1198\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1199\u001b[0m         timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[0;32m   1200\u001b[0m     )\n\u001b[0;32m   1201\u001b[0m \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n\u001b[0;32m   1202\u001b[0m     \u001b[39m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:1541\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[0;32m   1532\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[0;32m   1533\u001b[0m     method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mHEAD\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1534\u001b[0m     url\u001b[39m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1539\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[0;32m   1540\u001b[0m )\n\u001b[1;32m-> 1541\u001b[0m hf_raise_for_status(r)\n\u001b[0;32m   1543\u001b[0m \u001b[39m# Return\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:269\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    268\u001b[0m     message \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Client Error.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEntry Not Found for url: \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 269\u001b[0m     \u001b[39mraise\u001b[39;00m EntryNotFoundError(message, response) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[39melif\u001b[39;00m error_code \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGatedRepo\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;31mEntryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-647e06cf-000d0d244a6da8431eee15d6)\n\nEntry Not Found for url: https://huggingface.co/keras-io/ocr-for-captcha/resolve/main/preprocessor_config.json.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\python\\projects\\Upwork-Projects\\Sara RCNN\\rego_images\\predicting.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# import requests\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m processor \u001b[39m=\u001b[39m TrOCRProcessor\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mkeras-io/ocr-for-captcha\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m VisionEncoderDecoderModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mkeras-io/ocr-for-captcha\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# load image from the IAM dataset\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\processing_utils.py:184\u001b[0m, in \u001b[0;36mProcessorMixin.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\u001b[39mcls\u001b[39m, pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    155\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[39m    Instantiate a processor associated with a pretrained model.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[39m            [`~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`].\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 184\u001b[0m     args \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_get_arguments_from_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    185\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\processing_utils.py:228\u001b[0m, in \u001b[0;36mProcessorMixin._get_arguments_from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    226\u001b[0m         attribute_class \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(transformers_module, class_name)\n\u001b[1;32m--> 228\u001b[0m     args\u001b[39m.\u001b[39mappend(attribute_class\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs))\n\u001b[0;32m    229\u001b[0m \u001b[39mreturn\u001b[39;00m args\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\auto\\image_processing_auto.py:318\u001b[0m, in \u001b[0;36mAutoImageProcessor.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    315\u001b[0m trust_remote_code \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtrust_remote_code\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    316\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39m_from_auto\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 318\u001b[0m config_dict, _ \u001b[39m=\u001b[39m ImageProcessingMixin\u001b[39m.\u001b[39mget_image_processor_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    319\u001b[0m image_processor_class \u001b[39m=\u001b[39m config_dict\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mimage_processor_type\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    320\u001b[0m image_processor_auto_map \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\image_processing_utils.py:269\u001b[0m, in \u001b[0;36mImageProcessingMixin.get_image_processor_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    266\u001b[0m image_processor_file \u001b[39m=\u001b[39m IMAGE_PROCESSOR_NAME\n\u001b[0;32m    267\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    268\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m     resolved_image_processor_file \u001b[39m=\u001b[39m cached_file(\n\u001b[0;32m    270\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m    271\u001b[0m         image_processor_file,\n\u001b[0;32m    272\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    273\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    274\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    275\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    276\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    277\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    278\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    279\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    280\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[0;32m    281\u001b[0m     )\n\u001b[0;32m    282\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[0;32m    283\u001b[0m     \u001b[39m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[0;32m    284\u001b[0m     \u001b[39m# the original exception.\u001b[39;00m\n\u001b[0;32m    285\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\utils\\hub.py:463\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[39mif\u001b[39;00m revision \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m         revision \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmain\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    464\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m does not appear to have a file named \u001b[39m\u001b[39m{\u001b[39;00mfull_filename\u001b[39m}\u001b[39;00m\u001b[39m. Checkout \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    465\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mrevision\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m for available files.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    466\u001b[0m     )\n\u001b[0;32m    467\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    468\u001b[0m     \u001b[39m# First we try to see if we have a cached version (not up to date):\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     resolved_file \u001b[39m=\u001b[39m try_to_load_from_cache(path_or_repo_id, full_filename, cache_dir\u001b[39m=\u001b[39mcache_dir, revision\u001b[39m=\u001b[39mrevision)\n",
      "\u001b[1;31mOSError\u001b[0m: keras-io/ocr-for-captcha does not appear to have a file named preprocessor_config.json. Checkout 'https://huggingface.co/keras-io/ocr-for-captcha/main' for available files."
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "# import requests\n",
    "from PIL import Image\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"keras-io/ocr-for-captcha\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"keras-io/ocr-for-captcha\")\n",
    "\n",
    "# load image from the IAM dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image = Image.open(r\"C:\\python\\projects\\Upwork-Projects\\Sara RCNN\\rego_images\\data\\data - 20230403_141503-KP369-a7748e94-915f-48b6-91d1-8a56f3641129-cutout.jpg\").convert(\"RGB\")\n",
    "\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "generated_ids = model.generate(pixel_values)\n",
    "\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KP369'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.weight', 'encoder.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "InvalidSchema",
     "evalue": "No connection adapters were found for 'C:\\\\python\\\\projects\\\\Upwork-Projects\\\\Sara RCNN\\\\rego_images\\\\data\\\\data - 20230403_141503-KP369-a7748e94-915f-48b6-91d1-8a56f3641129-cutout.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidSchema\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\python\\projects\\Upwork-Projects\\Sara RCNN\\rego_images\\predicting.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#X21sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# load image from the IAM dataset\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#X21sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mpython\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mprojects\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mUpwork-Projects\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mSara RCNN\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mrego_images\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mdata - 20230403_141503-KP369-a7748e94-915f-48b6-91d1-8a56f3641129-cutout.jpg\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#X21sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(requests\u001b[39m.\u001b[39;49mget(url, stream\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39mraw)\u001b[39m.\u001b[39mconvert(\u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#X21sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m pixel_values \u001b[39m=\u001b[39m processor(image, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mpixel_values\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/projects/Upwork-Projects/Sara%20RCNN/rego_images/predicting.ipynb#X21sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mindustry, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m Mr. Brown commented icily. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m Let us have a\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39m\u001b[39mget\u001b[39m\u001b[39m\"\u001b[39m, url, params\u001b[39m=\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    585\u001b[0m }\n\u001b[0;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\sessions.py:695\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    692\u001b[0m hooks \u001b[39m=\u001b[39m request\u001b[39m.\u001b[39mhooks\n\u001b[0;32m    694\u001b[0m \u001b[39m# Get the appropriate adapter to use\u001b[39;00m\n\u001b[1;32m--> 695\u001b[0m adapter \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_adapter(url\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49murl)\n\u001b[0;32m    697\u001b[0m \u001b[39m# Start time (approximately) of the request\u001b[39;00m\n\u001b[0;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n",
      "File \u001b[1;32mc:\\Users\\LAPTOP WORLD\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\sessions.py:792\u001b[0m, in \u001b[0;36mSession.get_adapter\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[39mreturn\u001b[39;00m adapter\n\u001b[0;32m    791\u001b[0m \u001b[39m# Nothing matches :-/\u001b[39;00m\n\u001b[1;32m--> 792\u001b[0m \u001b[39mraise\u001b[39;00m InvalidSchema(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo connection adapters were found for \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mInvalidSchema\u001b[0m: No connection adapters were found for 'C:\\\\python\\\\projects\\\\Upwork-Projects\\\\Sara RCNN\\\\rego_images\\\\data\\\\data - 20230403_141503-KP369-a7748e94-915f-48b6-91d1-8a56f3641129-cutout.jpg'"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    TrOCRConfig,\n",
    "    TrOCRProcessor,\n",
    "    TrOCRForCausalLM,\n",
    "    ViTConfig,\n",
    "    ViTModel,\n",
    "    VisionEncoderDecoderModel,\n",
    ")\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "# TrOCR is a decoder model and should be used within a VisionEncoderDecoderModel\n",
    "# init vision2text model with random weights\n",
    "encoder = ViTModel(ViTConfig())\n",
    "decoder = TrOCRForCausalLM(TrOCRConfig())\n",
    "model = VisionEncoderDecoderModel(encoder=encoder, decoder=decoder)\n",
    "\n",
    "# If you want to start from the pretrained model, load the checkpoint with `VisionEncoderDecoderModel`\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "# load image from the IAM dataset\n",
    "url = r\"C:\\python\\projects\\Upwork-Projects\\Sara RCNN\\rego_images\\data\\data - 20230403_141503-KP369-a7748e94-915f-48b6-91d1-8a56f3641129-cutout.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "text = \"industry, ' Mr. Brown commented icily. ' Let us have a\"\n",
    "\n",
    "# training\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "labels = processor.tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "outputs = model(pixel_values, labels=labels)\n",
    "loss = outputs.loss\n",
    "round(loss.item(), 2)\n",
    "\n",
    "# inference\n",
    "generated_ids = model.generate(pixel_values)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data - 20230403_094217-6172f02f-75df-4e7c-b1dd...</td>\n",
       "      <td>GTe6s-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data - 20230403_094217-96b2f11d-9f00-434c-a838...</td>\n",
       "      <td>WW 8o5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data - 20230403_094217-a33648aa-fda3-40e9-bdf1...</td>\n",
       "      <td>GTéEZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data - 20230403_094217-ae807121-7d6a-43b2-8193...</td>\n",
       "      <td>Gia63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data - 20230403_094217-bbb7f34d-860e-4d1a-aa1d...</td>\n",
       "      <td>Qi a6ez_</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              images predicted label\n",
       "0  data - 20230403_094217-6172f02f-75df-4e7c-b1dd...          GTe6s-\n",
       "1  data - 20230403_094217-96b2f11d-9f00-434c-a838...          WW 8o5\n",
       "2  data - 20230403_094217-a33648aa-fda3-40e9-bdf1...           GTéEZ\n",
       "3  data - 20230403_094217-ae807121-7d6a-43b2-8193...           Gia63\n",
       "4  data - 20230403_094217-bbb7f34d-860e-4d1a-aa1d...        Qi a6ez_"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2 \n",
    "import pytesseract\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "images = []\n",
    "pred_labels = []\n",
    "data_path = r'C:\\python\\projects\\Upwork-Projects\\Sara RCNN\\rego_images\\dataS'\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "for img in os.listdir(data_path):\n",
    "\n",
    "    images.append(img)\n",
    "\n",
    "    img = cv2.imread(os.path.join(data_path,img))\n",
    "\n",
    "\n",
    "    # Adding custom options\n",
    "    custom_config = r'--oem 3 --psm 6'\n",
    "\n",
    "    pred_labels.append(pytesseract.image_to_string(img, config=custom_config).replace('\\n',''))\n",
    "\n",
    "    print('-',end='')\n",
    "\n",
    "df = pd.DataFrame({'images': images, 'predicted label': pred_labels})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('predictionSS.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
